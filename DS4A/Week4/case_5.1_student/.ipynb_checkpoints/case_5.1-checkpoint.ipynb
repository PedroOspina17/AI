{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we prepare data for use with an analytics platform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "\n",
    "In this case, we will introduce an uncleaned dataset - the type of dataset that you will most likely be working with in a job setting. We hope to provide you with a framework for tackling some common problems of unclean datasets (missing values, typos, etc.) as well as get you accustomed to integrating the Jupyter environment with other data environments (namely, AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context.** You are a data science consultant for a bike share company. The company has hundreds of thousands of users and has been collecting data about trips taken on each of their bikes. Since the dataset collected is quite large and increasing by the day, they have subscribed to a new analytics platform which gives them information and insights when they feed trip data into it. However, the analytics platform requires the collected data to be cleaned and converted into a certain format, for which the client requires your help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem.** Your task is to transform the raw data the client has into a format that can be fed into their analytics platform, as well as add additional features that the client wants to see in their platform, which are mentioned below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Context.** The data provided by the client is spread across 2 files. The first file is a CSV file that contains all the trip data, with features such as trip time, start and end stations, bike number, and the user details like whether it is a registered or a casual user, etc. The second file is a JSON file containing details about the stations they own. In addition to cleaning their existing data, the client wants you to add the following features:\n",
    "\n",
    "1. Generate a unique ID for each trip\n",
    "2. For each trip, calculate its duration, and based on this generate 4 more columns: `start_hour`, `end_hour`, `start_weekday`, `end_weekday`\n",
    "3. For each trip, create new columns for  age of the user, start and end coordinates, and municipal/city details of the start and end stations\n",
    "\n",
    "In this case, you will: (1) fetch the raw data from Amazon S3 and take a subset for local use (because the dataset is quite large); (2) use common sense to judge likely use cases of the data and clean it; (3) add new features to the data based on client request; and finally (4) upload the data wrangling scripts we have developed as a Jupyter Notebook to Amazon EC2 so we can apply it to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data from S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files are provided to you via an Amazon S3 bucket. To access the bucket, we've provided you with an Access Key and Secret Key with limited read-only permission on the single bucket of interest. \n",
    "\n",
    "Our first step is to copy the file from S3 to your local machine. We will use the [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) library to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if output/ directory exists and if it does not, create it\n",
    "if not os.path.exists('output'):\n",
    "    os.makedirs('output')\n",
    "\n",
    "# Setting up s3 client\n",
    "AWS_ACCESS_KEY = 'AKIAVOW4GLZ274D6QRDP'\n",
    "AWS_SECRET_ACCESS_KEY = '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'\n",
    "S3_BUCKET_NAME = 'data-wrangling-case'\n",
    "\n",
    "s3_client = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    ")\n",
    "\n",
    "s3_bucket = s3_client.Bucket(S3_BUCKET_NAME)\n",
    "local_folder = '.'\n",
    "\n",
    "    \n",
    "# Pull the contents from the data folder into the local path\n",
    "for obj in s3_bucket.objects.all():\n",
    "    local_file = os.path.join(local_folder, obj.key)\n",
    "    s3_bucket.download_file(obj.key, local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now copied to the local folder. Let's read the data using the [read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method.\n",
    "\n",
    "It is important to be more careful and diligent in the initial EDA stages when working with unclean data since good EDA is often essential for finding the tiny clues that inform you of the proper changes to make to unclean data. A common example would be a repetitive typo, such as 1% of the rows have \"New York\" spelled as \"New Yok\". We can use a combination of `groupby()` and `count()` in order to get a summary of the different values in a particular column, and notice that there are some common misspelled proper nouns.\n",
    "\n",
    "Furthermore, it can also be a good habit to shuffle the data and sift through a few hundred/thousand rows manually to get a good \"feel\" for the data. While this may seem excessive, you make catch an insight that will dramatically accelerate your data science process and that is definitely worth the extra time and effort. Learn to love manually digging into the data if you want to be a data professional.\n",
    "    \n",
    "Note that we are passing a parameter `nrows=10000` so that we only read in the first 10000 rows in the file (as mentioned earlier, we are working with a subset of the data on our local machine as the entire dataset is quite large):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>start_date</th>\n",
       "      <th>start_station</th>\n",
       "      <th>end_date</th>\n",
       "      <th>end_station</th>\n",
       "      <th>bike_nr</th>\n",
       "      <th>subsc_type</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closed</td>\n",
       "      <td>7/28/2011 10:12:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7/28/2011 10:12:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>B00468</td>\n",
       "      <td>Registered</td>\n",
       "      <td>'97217</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Closed</td>\n",
       "      <td>7/28/2011 10:21:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7/28/2011 10:25:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>B00554</td>\n",
       "      <td>Registered</td>\n",
       "      <td>'02215</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Closed</td>\n",
       "      <td>7/28/2011 10:33:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7/28/2011 10:34:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>B00456</td>\n",
       "      <td>Registered</td>\n",
       "      <td>'02108</td>\n",
       "      <td>1943.0</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Closed</td>\n",
       "      <td>7/28/2011 10:35:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7/28/2011 10:36:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>B00554</td>\n",
       "      <td>Registered</td>\n",
       "      <td>'02116</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Closed</td>\n",
       "      <td>7/28/2011 10:37:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7/28/2011 10:37:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>B00554</td>\n",
       "      <td>Registered</td>\n",
       "      <td>'97214</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status          start_date  start_station            end_date  end_station  \\\n",
       "0  Closed  7/28/2011 10:12:00           23.0  7/28/2011 10:12:00         23.0   \n",
       "1  Closed  7/28/2011 10:21:00           23.0  7/28/2011 10:25:00         23.0   \n",
       "2  Closed  7/28/2011 10:33:00           23.0  7/28/2011 10:34:00         23.0   \n",
       "3  Closed  7/28/2011 10:35:00           23.0  7/28/2011 10:36:00         23.0   \n",
       "4  Closed  7/28/2011 10:37:00           23.0  7/28/2011 10:37:00         23.0   \n",
       "\n",
       "  bike_nr  subsc_type zip_code  birth_date  gender  \n",
       "0  B00468  Registered   '97217      1976.0    Male  \n",
       "1  B00554  Registered   '02215      1966.0    Male  \n",
       "2  B00456  Registered   '02108      1943.0    Male  \n",
       "3  B00554  Registered   '02116      1981.0  Female  \n",
       "4  B00554  Registered   '97214      1983.0  Female  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_folder = '.'\n",
    "# Read the data file and take a look at the data\n",
    "df = pd.read_csv(os.path.join(local_folder, 'trips.csv'), nrows=10000)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, the first step we ought to take with a new dataset is to familiarize ourselves with it. Let's read through the columns present in the dataset, find out how the data is spread out across columns, etc. This will also give us a sense of the obvious cleaning steps to be performed on each column present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['status', 'start_date', 'start_station', 'end_date', 'end_station',\n",
       "       'bike_nr', 'subsc_type', 'zip_code', 'birth_date', 'gender'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the columns present\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of available features is as follow:\n",
    "\n",
    "1. **status:** Status of the trip (\"Ongoing\", \"Closed\")\n",
    "2. **start_date:** Start time of the trip\n",
    "3. **start_station:** id of the station from which this trip started\n",
    "4. **end_date:** End time of the trip\n",
    "5. **end_station:** id of the station from which this trip ended\n",
    "6. **bike_nr:** The unique identifier of the bike used in this trip\n",
    "7. **subsc_type:** Subscription type of the user (\"Registered\", \"Casual\")\n",
    "8. **zip_code:** If it is a registered user, their zipcode\n",
    "9. **birth_date** If it is a registered user, their date of birth\n",
    "10. **gender** If it is a registered user, their gender (\"Male\", \"Female\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the data cleaning process. As discussed in the intro Python cases, one of the first steps is to deal with null or missing values. However, previous cases only gave a passing treatment of these and resulted in dropping the rows containing null values entirely. Here, we will be more nuanced and look at ways that null or missing values can be replaced with more understandable and logical values. \n",
    "\n",
    "In the real world, dealing with null values definitely has an artistic component, and you may have to test methods to see what works best. As always, domain knowledge is crucial in selecting what method to go forward with. For this program, we will introduce some of the simpler and more intuitive approaches, but recommend students to be as creative as they can when resolving null values while still adhering to logic.\n",
    "\n",
    "Generally, null values in a specific column are dealt with in one of the following ways:\n",
    "\n",
    "1. Any row containing a null value for that column is removed\n",
    "2. If that column's feature is a string, null values are replaced with a \"Not found\" string. If that column's feature is a number, null values are replaced either with 0 or the mean/median of the available values in the column\n",
    "3. Null values are replaced with an interpolated value based on the data present in other rows\n",
    "\n",
    "Let's start by taking a look at the list of columns that have null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status           False\n",
       "start_date       False\n",
       "start_station    False\n",
       "end_date         False\n",
       "end_station       True\n",
       "bike_nr           True\n",
       "subsc_type       False\n",
       "zip_code          True\n",
       "birth_date        True\n",
       "gender            True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 5 columns that have null values. We need to decide, based on the importance of each column, whether we will be applying a blunt instrument and remove the rows that have a particular column as null, or if we will be more nuanced and replace the null values in that column with a replacement value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "`bike_nr` is one of the columns with missing values. Which of the above three methods do you think is most appropriate?\n",
    "\n",
    "**Answer.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that at best we can only fill `bike_nr` with a meaningless filler value, it may be tempting to remove rows without `bike_nr` entirely. However, removing a row entirely tends to be the nuclear option and in the absence of a very clear and limited use case for the dataset going forward, it may result in us regretting our choice once we realize that some data we cut out might actually be useful. For example, if in some future analyses the client cares a fair amount about aggregate statistics relating to many or all the trips, then a specific `bike_nr` value is not important and we can still get useful analysis by replacing each `bike_nr` with a \"Not Found\", whereas we could well be missing that analysis if we cut out too much data. Thus, option 2 is the best answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above, let's now replace null values for `bike_nr` with \"Not found\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bike_nr'].fillna('Not Found', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next column we can concentrate on is `gender`. We know that `gender` can have only one of the 2 values: \"Male\" or \"Female\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Suppose that one thing (but not the only thing) our client cares about is a rough idea of how the number of male and female riders has changed over time. Which of the above three methods would be most appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how do we go about doing this? Well, we can use the [interpolate()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html) function in `pandas`. The `interpolate()` function uses linear interpolation, which is a mathematical method for filling in unknown points based on building a linear regression model on the non-missing points. We can then use this model to estimate the values of the missing points. This is very different from substituting null values with random or meaningless values, as it preserves aspects of the distribution of the data, which can be very important for certain analyses.\n",
    "\n",
    "But gender is a string; how can we apply a mathematical model to a string? Well luckily, gender can only take on two values, so we can convert it to a `category` type and then run the interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gender to a category type\n",
    "df['gender'] = df['gender'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting `gender` to be a category column, let's see what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Female', 'Male'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gender'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "9995   -1\n",
       "9996   -1\n",
       "9997   -1\n",
       "9998   -1\n",
       "9999   -1\n",
       "Length: 10000, dtype: int8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the codes assigned to the values present in the DataFrame.\n",
    "df['gender'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there are 3 codes here: -1, 0, 1. But in the categories list, there are only 2 values: \"Male\" and \"Female\". This is because -1 represents the NaN values. In order to interpolate the values, we need to convert the -1 into actual NaN, as interpolation works only on NaN values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       1.0\n",
       "2       1.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "9995    NaN\n",
       "9996    NaN\n",
       "9997    NaN\n",
       "9998    NaN\n",
       "9999    NaN\n",
       "Length: 10000, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The below code replaces the value -1 with NaN.\n",
    "gender = df['gender'].cat.codes.replace(-1, np.nan)\n",
    "gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now call the interpolate function that actually fills the NaN values with either a 0 or 1\n",
    "gender = gender.interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Male\n",
       "1         Male\n",
       "2         Male\n",
       "3       Female\n",
       "4       Female\n",
       "         ...  \n",
       "9995      Male\n",
       "9996      Male\n",
       "9997      Male\n",
       "9998      Male\n",
       "9999      Male\n",
       "Name: gender, Length: 10000, dtype: category\n",
       "Categories (2, object): [Female, Male]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender = gender.astype(int).astype('category')\n",
    "gender = gender.cat.rename_categories(df['gender'].cat.categories)\n",
    "df['gender'] = gender\n",
    "df['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above snippet, we are converting the interpolated values into category type and then replacing the category names with the names from the existing DataFrame column. So now the gender column will contain \"Male\" and \"Female\" as its values (rather than 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Come up with a proper replacement scheme for null values in `zip_code`, `birth_date`, and `end_station`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting erroneous string values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the null values in the dataset, the next process we usually do is to check for and deal with erroneous values in the dataset. For string columns, this can manifest in a few ways:\n",
    "\n",
    "1. Unnecessary spaces at the start or end of the string\n",
    "2. Mixed case (e.g. \"value 1\" and \"Value 1\")\n",
    "3. Spelling mistakes\n",
    "\n",
    "However, this can be quite difficult to clean for in practice. In particular, number 3 (spelling mistakes) are nearly impossible to find if that string column is meant to be free-form; any legitimate English word would be a legal entry, and you would have to cross-check each entry against the entire English dictionary! Situations like this are why sometimes a manual sifting of thousands of rows can be beneficial. It is a lot easier for the naked eye to catch and error and then write code for systematically cleaning up that type of error from the entire dataset, than to guess at what type of error even exists and writing code for that unsubstantiated guess.\n",
    "\n",
    "Luckily, certain types of string columns are far easier to deal with. For example, `subscription_type`, `gender` and `status` are categorical; i.e. their values are supposed to come from a defined set of options. This means that we can do some basic data summarizing to see whether they contain any values they are not supposed to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subscription Type:  ['Registered' 'Casual' 'registered' ' Casual' 'Casual ' 'casual'\n",
      " ' Registered']\n",
      "Gender: [Male, Female]\n",
      "Categories (2, object): [Male, Female]\n",
      "Status: ['Closed']\n"
     ]
    }
   ],
   "source": [
    "print(\"Subscription Type: \", df['subsc_type'].unique())\n",
    "print(\"Gender:\", df['gender'].unique())\n",
    "print(\"Status:\", df['status'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data shows that `gender` and `status` only have 2 values, as expected.\n",
    "\n",
    "However, in the `subscription_type` column, you will see that there are 3 `Regular`s and 3 `Casual`s. This is because of leading and trailing spaces. You can also see that this column has `regular` and `casual` as values. Though `Regular` and `regular` are same, they are identified as 2 different values because of their mixed case.\n",
    "\n",
    "Let's go ahead and fix both of these issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Registered', 'Casual', 'registered', 'casual'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's fix the space issue. We will use the strip function to remove the leading and trailing spaces \n",
    "# for each row in the subsc_type column\n",
    "df['subsc_type'] = df['subsc_type'].str.strip()\n",
    "\n",
    "df['subsc_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['REGISTERED', 'CASUAL'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subsc_type'] = df['subsc_type'].str.upper()\n",
    "\n",
    "df['subsc_type'].unique()\n",
    "\n",
    "# We have cleansed both the gender and subscription type columns to contain proper values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, data scientists will apply the [strip()](https://www.w3schools.com/python/ref_string_strip.asp) and [upper()](https://www.w3schools.com/python/ref_string_upper.asp) (or [lower()](https://www.w3schools.com/python/ref_string_lower.asp), either is fine) functions even to free-form, non-categorical string columns to avoid any unintentional duplicates (though as mentioned earlier, spelling mistakes need to be dealt with separately). An example of this is if you had a column specifying the city someone was from – you would not want \"New York City, New York\" to be treated differently from \"new york city, New York\".\n",
    "\n",
    "There are many ways that values within a column can be erroneous, and we have only covered a few of them here. In future cases, you will learn about other ways erroneous values can creep into your dataset (say, in numeric or `datetime` columns) and how to deal with them. For now, let's move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a unique ID for each trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get going on the specifics that the client wants. The first request relates to unique identifiers. When generating unique identifiers for datasets like this, we should make sure the generation process is idempotent (i.e. the same ID should be generated for each trip no matter how many times you run the script). The idempotency is required because there may be chances that the same trip is input into this tool multiple times. For example, the customer first uploads the data set for the first week of the month (may be for testing purposes, or based on data availability, etc.) and then uploads the data for the entire month. Now if the same trip is assigned different IDs on each run, then it might result in the analytics platform interpreting this as two different trips and this will skew the analysis.\n",
    "\n",
    "ID generation is a subset of **feature engineering** which we will cover in greater detail in future cases. Here, we just want to introduce you to the idea of combining/manipulating various parameters to create new ones that may greatly help you solve your data science problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Describe how you would generate a unique ID per trip while guaranteeing idempotence, then write code to do this.\n",
    "\n",
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip timing details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second requirement of your client is to create a few additional features. These are also a subset of feature engineering. The first of these is trip duration. Now, trip duration is defined as `end time - start time`, so naturally we would consider using those columns. But if you take a look at the start and end date columns, you can see that they are not listed as `datetime` columns; rather, they are just shown as strings. In order to calculate the trip duration, we will have to convert the strings into [datetime](https://www.w3schools.com/python/python_datetime.asp) objects. We will be using the [pd.to_datetime()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) function to convert all values of a column into date objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-07-28 10:12:00</td>\n",
       "      <td>2011-07-28 10:12:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-07-28 10:21:00</td>\n",
       "      <td>2011-07-28 10:25:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-07-28 10:33:00</td>\n",
       "      <td>2011-07-28 10:34:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-07-28 10:35:00</td>\n",
       "      <td>2011-07-28 10:36:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-07-28 10:37:00</td>\n",
       "      <td>2011-07-28 10:37:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>2011-08-07 20:38:00</td>\n",
       "      <td>2011-08-07 21:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>2011-08-07 20:38:00</td>\n",
       "      <td>2011-08-07 21:04:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>2011-08-07 20:42:00</td>\n",
       "      <td>2011-08-07 20:59:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>2011-08-07 20:42:00</td>\n",
       "      <td>2011-08-07 20:58:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>2011-08-07 20:44:00</td>\n",
       "      <td>2011-08-07 21:17:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              start_date            end_date\n",
       "0    2011-07-28 10:12:00 2011-07-28 10:12:00\n",
       "1    2011-07-28 10:21:00 2011-07-28 10:25:00\n",
       "2    2011-07-28 10:33:00 2011-07-28 10:34:00\n",
       "3    2011-07-28 10:35:00 2011-07-28 10:36:00\n",
       "4    2011-07-28 10:37:00 2011-07-28 10:37:00\n",
       "...                  ...                 ...\n",
       "9995 2011-08-07 20:38:00 2011-08-07 21:04:00\n",
       "9996 2011-08-07 20:38:00 2011-08-07 21:04:00\n",
       "9997 2011-08-07 20:42:00 2011-08-07 20:59:00\n",
       "9998 2011-08-07 20:42:00 2011-08-07 20:58:00\n",
       "9999 2011-08-07 20:44:00 2011-08-07 21:17:00\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then with the remaining values, convert them to datetime objects\n",
    "df['start_date'] = pd.to_datetime(df['start_date'], format='%m/%d/%Y %H:%M:%S')\n",
    "df['end_date'] = pd.to_datetime(df['end_date'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# You can now see that the start date and end date are converted to datetime objects.\n",
    "df[['start_date', 'end_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With start date and end date being datetime objects now, it is easy to calculate the trip duration for each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trip_duration'] = (df['end_date'] - df['start_date'])\n",
    "\n",
    "df['trip_duration'] = df['trip_duration'].apply(lambda x: x.seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          0\n",
       "1        240\n",
       "2         60\n",
       "3         60\n",
       "4          0\n",
       "        ... \n",
       "9995    1560\n",
       "9996    1560\n",
       "9997    1020\n",
       "9998     960\n",
       "9999    1980\n",
       "Name: trip_duration, Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trip_duration column will contain the duration of the trip in seconds\n",
    "df['trip_duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Generate the 4 other time-related columns the client wants: `start_weekday`, `start_hour`, `end_weekday`, `end_hour`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Replicate the process we used to calculate trip duration in order to calculate the age of the registered users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding stations data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is add municipal details as well as coordinates of the start and end stations. To get the latitude and longitude of each station, we will be using the address of the stations present in the [JSON](https://www.w3schools.com/js/js_json_intro.asp) file provided along with the CSV data.\n",
    "\n",
    "Notice that we are combining information from different files (and also different file types) to progress in our investigation of this dataset. This is extremely common and in the professional world you may have to utilize different files from very different sources with varying degrees of reliability/accuracy to solve problems. You may have to clean these accessory datasets beforehand just to make them usable. In fact, most data professionals spend much more time on cleaning data than on modeling.\n",
    "\n",
    "The JSON file contains the station ID, station name, and municipal. We will generate the address by concatenating the station and municipal fields and then use the `Nominatim`, which is a geocoding library provided and maintained by OpenStreetMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the trip data in place, next we can pull the JSON data of the stations.\n",
    "stations_data = []\n",
    "with open(os.path.join(local_folder, 'stations.json')) as f:\n",
    "    stations_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colleges of the Fenway,Boston\n",
      "42.3406888\n",
      " Berkeley St.,Boston\n",
      "42.3473952\n",
      "Northeastern U ,Boston\n",
      "42.3400454\n",
      " Joy St.,Boston\n",
      "42.3593467\n",
      "Fan Pier,Boston\n",
      "42.35260065\n",
      "Union Square ,Boston\n",
      "42.3537768\n",
      "Agganis Arena ,Boston\n",
      "42.351313\n",
      "B.U. Central ,Boston\n",
      "-34.8484354\n",
      "Longwood Ave ,Boston\n",
      "42.3402705\n",
      "ion ,Boston\n",
      "46.7402641\n",
      "Boston Medical Center ,Boston\n",
      "42.334158349999996\n",
      "HMS ,Boston\n",
      "42.337171\n",
      "e ,Boston\n",
      "42.3602534\n",
      "Back Bay ,Boston\n",
      "42.3507067\n",
      "Harvard University Housing ,Boston\n",
      "42.3644647\n",
      "e ,Boston\n",
      "42.3602534\n",
      "Buswell Park,Boston\n",
      "42.347241\n",
      "ion ,Boston\n",
      "46.7402641\n",
      "Prudential Center ,Boston\n",
      "42.347087650000006\n",
      "ion ,Boston\n",
      "46.7402641\n",
      "Mayor Thomas M. Menino ,Boston\n",
      "42.3780804\n",
      "Seaport Square ,Boston\n",
      "42.3502036\n",
      "Tremont St ,Boston\n",
      "42.3536223\n",
      " Waltham St.,Boston\n",
      "42.342256\n",
      "ion,Boston\n",
      "46.7402641\n",
      "ion Lab ,Boston\n",
      "Brigham Cir ,Boston\n",
      "42.334321\n",
      "Seaport Hotel,Boston\n",
      "42.3502036\n",
      "Landmark Centre,Boston\n",
      "42.3448548\n",
      "Kenmore Sq ,Boston\n",
      "42.3489517\n",
      " Brookline Ave,Boston\n",
      "42.3460711\n",
      "Summer St. ,Boston\n",
      "42.349216\n",
      "Boston Public Library ,Boston\n",
      "42.3493269\n",
      "New Balance ,Boston\n",
      "42.35673355\n",
      "TD Garden ,Boston\n",
      "42.36628265\n",
      " Rutland St.,Boston\n",
      "42.3387682\n",
      "Lewis Wharf ,Boston\n",
      "42.3634593\n",
      "Packard's Corner ,Boston\n",
      "42.3517566\n",
      " Arlington St.,Boston\n",
      "42.3536766\n",
      "Rowes Wharf ,Boston\n",
      "42.35644675\n",
      "Faneuil Hall ,Boston\n",
      "42.3600335\n",
      " Boylston St.,Boston\n",
      "42.3512497\n",
      "Christian Science Plaza,Boston\n",
      "42.34434735\n",
      " Hanover St.,Boston\n",
      "42.3625956\n",
      "Post Office Square,Boston\n",
      "42.3575068\n",
      " Charles St.,Boston\n",
      "42.3609631\n",
      "Boylston St ,Boston\n",
      "42.3512497\n",
      " Lenox St.,Boston\n",
      "42.3378396\n",
      "Newbury St ,Boston\n",
      "42.3487908\n",
      "Beacon St ,Boston\n",
      "42.3329651\n",
      "Tremont St ,Boston\n",
      "42.3536223\n",
      "Boylston ,Boston\n",
      "42.3529861\n",
      "Dudley Square,Boston\n",
      "42.350500350000004\n",
      " Mass. Ave.,Boston\n",
      "42.3602534\n",
      "The Esplanade ,Boston\n",
      "42.3567879\n",
      "own G,Boston\n",
      "41.2311096\n",
      "Charles Circle ,Boston\n",
      "42.3531916\n",
      " Fairfield,Boston\n",
      "41.2781512\n",
      "Longwood Ave,Boston\n",
      "42.3402705\n",
      " Gillette Park,Boston\n",
      "42.3442224\n",
      "Congress ,Boston\n",
      "42.360400999999996\n",
      "Boston Convention & Exhibition Center,Boston\n",
      "42.345590200000004\n",
      "Allston Green District ,Boston\n",
      "42.3803376\n",
      " Mass Ave ,Cambridge\n",
      "42.3750997\n",
      " Mass Ave ,Cambridge\n",
      "42.3750997\n",
      "Coolidge Corner ,Brookline\n",
      "42.3421524\n",
      " Bennett St ,Cambridge\n",
      "42.3724748\n",
      "Conway Park ,Somerville\n",
      "42.383092000000005\n",
      "One Broadway ,Cambridge\n",
      "42.362804800000006\n",
      " Br,Cambridge\n",
      "52.2034823\n",
      " Mass Ave,Cambridge\n",
      "42.3750997\n",
      " Mass Ave ,Cambridge\n",
      "42.3750997\n",
      "Central Sq Post Office ,Cambridge\n",
      "51.7705695\n",
      "Somerville City Hall,Somerville\n",
      "42.387194300000004\n",
      "Union Square ,Somerville\n",
      "42.3797735\n",
      " Washington ,Somerville\n",
      "38.5217907\n",
      "a Center ,Cambridge\n",
      "52.2034823\n",
      "Boylston St ,Boston\n",
      "42.3512497\n",
      "Brookline Town Hall ,Brookline\n",
      "42.33402015\n",
      "South Bay Plaza,Boston\n",
      "CambridgeSide Galleria ,Cambridge\n",
      "42.3681723\n",
      "ion ,Boston\n",
      "46.7402641\n",
      "Brookline Village ,Brookline\n",
      "42.332592\n",
      "Harvard University Housing ,Cambridge\n",
      "42.366621\n",
      " Vellucci Plaza ,Cambridge\n",
      "42.37221495\n",
      " Mass Ave ,Cambridge\n",
      "42.3750997\n",
      "ion ,Cambridge\n",
      "43.4066968\n",
      " Hampshire St ,Cambridge\n",
      "42.3728398\n",
      "University of Massachusetts Boston,Boston\n",
      "42.3140821\n",
      "JFK ,Boston\n",
      "42.3205447\n",
      "Charlestown ,Boston\n",
      "42.3778749\n",
      "Cambridge St ,Cambridge\n",
      "42.370424\n",
      " Broadway ,Cambridge\n",
      "42.3692865\n",
      " DeWolfe St ,Cambridge\n",
      "42.3700082\n",
      "Charlestown ,Boston\n",
      "42.3778749\n",
      "Mayor Thomas M. Menino ,Boston\n",
      "42.3780804\n",
      "Dudley Square,Boston\n",
      "42.350500350000004\n",
      "BIDMC ,Boston\n",
      "42.345733\n",
      "Boston Medical Center ,Boston\n",
      "42.334158349999996\n",
      " Fairfield,Boston\n",
      "41.2781512\n",
      "ion ,Boston\n",
      "46.7402641\n",
      "Mt Pleasant Ave ,Boston\n",
      "42.3258913\n",
      " Dorchester St,Boston\n",
      "42.3367276\n",
      "South Boston Library ,Boston\n",
      "42.3334312\n",
      " Columbia Rd,Boston\n",
      "42.3295129\n",
      "Upham's Corner ,Boston\n",
      "42.3167648\n",
      "New Balance ,Boston\n",
      "42.35673355\n",
      "TD Garden ,Boston\n",
      "42.36628265\n",
      "Franklin St. ,Boston\n",
      "42.355738\n",
      "Charles Circle ,Boston\n",
      "42.3531916\n",
      "TD Garden ,Boston\n",
      "42.36628265\n",
      "ion Hospital ,Boston\n",
      "46.7402641\n",
      " Beacon St,Boston\n",
      "42.3329651\n",
      " India St,Boston\n",
      "42.3578825\n",
      " Vine St.,Boston\n",
      "42.3767654\n",
      "New Balance Store ,Boston\n",
      "42.35673355\n",
      "JP Monument ,Boston\n",
      "JP Centre ,Boston\n",
      "35.7039099\n",
      " Barbara St,Boston\n",
      "42.3210245\n",
      " Columbus Ave,Boston\n",
      "42.3183474\n",
      "Green St T,Boston\n",
      "42.310351\n",
      " Centre St,Boston\n",
      "42.3568331\n",
      " Washington St. ,Brookline\n",
      "42.3317493\n",
      " Harvard St. ,Brookline\n",
      "42.336939\n",
      " Main St,Cambridge\n",
      "42.3617803\n",
      "Harvard University River Houses ,Cambridge\n",
      " Quincy St ,Cambridge\n",
      "42.3755809\n",
      " Magazine St,Cambridge\n",
      "42.3578903\n",
      "Harvard University ,Cambridge\n",
      "42.36790855\n",
      " Shepard St ,Cambridge\n",
      "42.3813691\n",
      "Mass Ave ,Cambridge\n",
      "42.3750997\n",
      "359 Broadway ,Cambridge\n",
      "42.3709378\n",
      "Biogen Idec ,Cambridge\n",
      "42.36498625\n",
      "ion,Cambridge\n",
      "43.4066968\n",
      "Wilson Square,Somerville\n",
      "42.38537435\n",
      "Davis Square,Somerville\n",
      "42.3963615\n",
      "Ball Square,Somerville\n",
      "42.3999113\n",
      "Powder House Circle,Somerville\n",
      "Packard Ave ,Somerville\n",
      "42.406964\n",
      " Highland Ave ,Somerville\n",
      "42.3960546\n",
      " 239 Holland St,Somerville\n",
      "42.40262128571428\n",
      " Cutter St,Somerville\n",
      "42.3876075\n"
     ]
    }
   ],
   "source": [
    "# For each of the stations, let's get the latitude and longitude for each of the stations, based on the address\n",
    "geolocator = Nominatim(user_agent=\"Address Predictor\", timeout=10)\n",
    "for station in stations_data:\n",
    "    address = station['station'] + ',' +  station['municipal']\n",
    "    print(address)\n",
    "    lat_long = geolocator.geocode(address)\n",
    "    if lat_long:\n",
    "        print(lat_long.latitude)\n",
    "        location = ','.join([str(lat_long.latitude), str(lat_long.longitude)])\n",
    "    else:\n",
    "        location = 'None,None'\n",
    "    station['coordinates'] = location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7:\n",
    "\n",
    "`stations_data` now contains the coordinates for each station. Use this to add the start and end coordinates to each trip in the trips data. Add the municipal data for each trip as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the coordinates into separate latitude and longitude columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated `start_coordinates` and `end_coordinates` for each trip, but they are currently residing in the same column separated by a comma. The client has asked for them to be in separate columns. This can also be viewed as a subset of feature engineering as we are creating new parameters from a single parameter. This is less common than the feature generation example we showed earlier, but it is still relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_latitude</th>\n",
       "      <th>start_longitude</th>\n",
       "      <th>end_latitude</th>\n",
       "      <th>end_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "      <td>42.3780804</td>\n",
       "      <td>-71.04848834660194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>42.3575068</td>\n",
       "      <td>-71.0563378</td>\n",
       "      <td>42.3634593</td>\n",
       "      <td>-71.0500799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>42.3575068</td>\n",
       "      <td>-71.0563378</td>\n",
       "      <td>42.3634593</td>\n",
       "      <td>-71.0500799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>42.3507067</td>\n",
       "      <td>-71.0797297</td>\n",
       "      <td>-34.8484354</td>\n",
       "      <td>-58.2756612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>42.3507067</td>\n",
       "      <td>-71.0797297</td>\n",
       "      <td>-34.8484354</td>\n",
       "      <td>-58.2756612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>42.3402705</td>\n",
       "      <td>-71.1097022</td>\n",
       "      <td>42.34434735</td>\n",
       "      <td>-71.08350216721783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     start_latitude     start_longitude end_latitude       end_longitude\n",
       "0        42.3780804  -71.04848834660194   42.3780804  -71.04848834660194\n",
       "1        42.3780804  -71.04848834660194   42.3780804  -71.04848834660194\n",
       "2        42.3780804  -71.04848834660194   42.3780804  -71.04848834660194\n",
       "3        42.3780804  -71.04848834660194   42.3780804  -71.04848834660194\n",
       "4        42.3780804  -71.04848834660194   42.3780804  -71.04848834660194\n",
       "...             ...                 ...          ...                 ...\n",
       "9995     42.3575068         -71.0563378   42.3634593         -71.0500799\n",
       "9996     42.3575068         -71.0563378   42.3634593         -71.0500799\n",
       "9997     42.3507067         -71.0797297  -34.8484354         -58.2756612\n",
       "9998     42.3507067         -71.0797297  -34.8484354         -58.2756612\n",
       "9999     42.3402705         -71.1097022  42.34434735  -71.08350216721783\n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Notice the expand=True parameter. That is required since we are assigning the output of the split function to 2 columns\n",
    "df[['start_latitude', 'start_longitude']] = df['start_coordinates'].str.split(',', expand=True)\n",
    "df[['end_latitude', 'end_longitude']] = df['end_coordinates'].str.split(',', expand=True)\n",
    "df[['start_latitude', 'start_longitude', 'end_latitude', 'end_longitude']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Jupyter notebook in an EC2 instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to copy the notebook file from the local machine to the EC2 instance. It can be done via an scp command, or using winSCP. There are different ways to SCP (secure copy) the file into the server:\n",
    "\n",
    "1. If you are using a Linux or a Mac, type the following command to copy the file:\n",
    "\n",
    "```\n",
    "scp -i <path_to_pem_file> </path/to/jupyter/notebook> <username>@<EC2IP>:</destination/path>\n",
    "```\n",
    "\n",
    "2. If you are using Windows, you can either use WinSCP - https://winscp.net/eng/index.php or if you are using Putty, you can use the pscp command:\n",
    "\n",
    "```\n",
    "pscp -i <path_to_ppk_file> <\\path\\to\\jupyter\\notebook\\> <username>@<EC2IP>:</destination/path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute on the entire dataset in the server\n",
    "\n",
    "Once the file is copied, you can now access the Jupyter Notebook from `http:server_ip:8889`. Once you are able to access the file, just remove the `nrows=10000` parameter from the `read_csv` function call, in order to read the entire dataset. You can now click on `Cells > Run All` in order to run the same set of steps on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this case, we cleaned up some messy client data and also added some new features based on their requests. We first took a small sample of the overall data so we could work with it locally. However, cleaning up the data was not merely some rote mechanical work. We had to use our common sense and judgment of likely use cases of this data in order to determine how to fix it. Additionally, we had to consider the potential impact of our changes and whether it would adversely impact potential uses of the data in the future, even if those were not likely uses at this time.\n",
    "\n",
    "After cleaning the data, we uploaded our cleaning scripts to Amazon EC2 so that they could be applied to the entire dataset.\n",
    "\n",
    "Data wrangling & cleaning is arguably the most important step of the entire data science process, without which we could have wrong or corrupt results. In practice, data scientists can spend upwards of 60 - 70% of their time on cleaning and organizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "When cleaning data, one very common problem is missing data. We learned a few ways of dealing with missing data and when these methods tend to be appropriate:\n",
    "\n",
    "1. Removing the rows with missing data entirely is a nuclear option that only makes sense when you have a clear and very limited use case for the dataset.\n",
    "2. Replacing the missing values with a meaningless filler like \"Not found\" or 0 makes sense if you need to preserve the other data in those rows but either cannot sensibly fill in or don't particularly care about the missing values in that specific column.\n",
    "3. Interpolating the missing values makes sense when you need to preserve elements of the underlying distribution of the data in that column.\n",
    "\n",
    "We also saw that for large datasets, working with a small subset of the data is a useful tool for gaining intuition about the dataset and rapidly iterating on the cleaning process. This intuition gathering element cannot be emphasized enough; in fact, many steps in our cleaning process involved elements of exploratory data analysis, via generating summary statistics. Running cleaning steps on the entirety of a very large dataset is time consuming and can bog down the EDA that is essential to cleaning. In future cases, you will see further evidence of EDA in action during wrangling & cleaning."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
