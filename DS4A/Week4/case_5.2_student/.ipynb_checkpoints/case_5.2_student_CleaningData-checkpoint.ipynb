{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we prepare Yelp data to answer business questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from shapely.geometry import Point, shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "In this case, we will introduce you to another uncleaned dataset for practice. We will focus on the nuances of cleaning the dataset parameter by parameter. Being able to dive into a single parameter and investigate its attributes will be crucial for modeling in later stages of the data science proceess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context.** [Yelp](https://www.yelp.com) is a very popular website, where anyone can write a review about restaurants, hotels, spas or any business. They have decided to start analysing the data and will use the results of the analysis to make decisions about new features to the service. They have now approached you, an independant data consultant with a requirement of cleaning the data they have and for you to help them get more context on the data.\n",
    "\n",
    "**Business Problem.** Your task is to go through the data, and make transformations or additions to the data based on their needs, which are listed below.\n",
    "\n",
    "**Analytical Context.** The client has shared three files with you containing details about the businesses and end-users on the service along with the reviews posted by these users. With this data, they would like you to do the following:\n",
    "\n",
    "1. For each business, find out the county and state that it is a part of\n",
    "2. For each business, calculate the total number of reviews received and the average star rating across those reviews\n",
    "3. For each business, calculate the total number of check-ins by hour of day, by day of week, and across all time\n",
    "    \n",
    "We will initially run these operations locally with a small subset of the data. We will then run the same set of operations in an EC2 instance for the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in all three data files provided and familiarize ourselves with the data. Note that we truncate our read to the first 10000 rows due to the sizes of the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('reviews.csv', nrows=1000)\n",
    "businesses = pd.read_csv('businesses.csv', nrows=1000)\n",
    "checkins = []\n",
    "with open('checkins.json', encoding='utf-8')  as f:\n",
    "    for row in f.readlines()[:1000]:\n",
    "        checkins.append(json.loads(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns present in each of the files are:\n",
    "\n",
    "1. **reviews**\n",
    "    * **review_id** - Unique identifier for the review\n",
    "    * **business_id** - Unique identifier of the business that is being reviewed\n",
    "    * **user_id** - Unique identifier of the user who posted the review\n",
    "    * **date** - The date and time of the review\n",
    "    * **star** - Star rating for the review\n",
    "    * **text** - Review text\n",
    "    * **cool** - Number of cool votes received for the review\n",
    "    * **funny** - Number of funny votes received for the review\n",
    "    * **useful** - Number of funny votes received for the review\n",
    "    \n",
    "    \n",
    "2. **businesses**\n",
    "    * **business_id** - Unique identifier of the business\n",
    "    * **name** - Name of the business\n",
    "    * **categories** - Categories associated with the business\n",
    "    * **latitude** - Location of the business (latitude)\n",
    "    * **longitude** - Location of the business (longitude)\n",
    "    * **review_count** - Number of reviews received for the business\n",
    "    * **stars** - Average star rating, rounded to half stars\n",
    "    \n",
    "    \n",
    "3. **checkins**\n",
    "    * **business_id** - Unique identifier of the business\n",
    "    * **date** - List of datetimes when users checked in to the business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling null values in `reviews`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, handling null values is a staple of cleaning datasets. Let's uncover the columns in the table `reviews` that contain null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date            True\n",
       "funny           True\n",
       "review_id      False\n",
       "stars           True\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that null values are present in the following columns: `date`, `stars`, `cool`, `funny`, and `useful`. Each of these columns need to be handled in a different way.\n",
    "\n",
    "In contrast to the previous case, here we will look at some advanced methods for handling null values that don't use normal `pandas` operations. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `date`\n",
    "\n",
    "Since the client has indicated they will likely be using the cleaned data for numerous analyses later on, removing rows with missing values or even writing in meaningless filler values will not work. So we need to figure out a sensible interpolation method.\n",
    "\n",
    "### Exercise 1:\n",
    "\n",
    "Given what we know about the data so far, describe a sensible interpolation method that we can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `star`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next feature which has null values to handle is `star`. `star` is a numeric value and can range from 1 to 5, where 1 represents the lowest rating and 5 the highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Describe an appropriate method for filling in the missing values of `star`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `cool`, `funny`, `useful` and revisiting interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply a similar logic to conclude that the `cool`, `funny`, and `useful` columns should also be filled with `NaN`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['cool'].fillna(np.nan, inplace=True)\n",
    "reviews['funny'].fillna(np.nan, inplace=True)\n",
    "reviews['useful'].fillna(np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's assume, for the sake of progress, that the inputs `cool`, `date`, `funny`, `useful`, and `text` ARE good predictors for `star`, and that the client is mostly interested in high-level properties of the distributions of these variables and how they correlate to each other. In such a case, we can go ahead and leverage the power of machine learning to predict a reasonable estimate for `star`. \n",
    "\n",
    "We will do so by using the [IterativeImputer](https://scikit-learn.org/stable/modules/impute.html#multivariate-feature-imputation) class of `scikit-learn` to fill the missing values in these columns. `IterativeImputer` fills one column after another by building models for each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date           False\n",
       "funny           True\n",
       "review_id      False\n",
       "stars           True\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "cool            True\n",
       "date           False\n",
       "funny           True\n",
       "review_id      False\n",
       "stars          False\n",
       "text           False\n",
       "useful          True\n",
       "user_id        False\n",
       "index          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['index'] = reviews.index\n",
    "imputer = IterativeImputer()\n",
    "imputed_df = pd.DataFrame(imputer.fit_transform(reviews[['index', 'stars']]))\n",
    "imputed_df\n",
    "imputed_df.columns = ['index', 'stars']\n",
    "imputed_df.index = reviews.index\n",
    "reviews[['stars']] = imputed_df['stars']\n",
    "reviews.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value has to be dynamically filled, similar to the cool and funny columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with erroneous values\n",
    "\n",
    "Of course, missing values are not the only problem in datasets – erroneous values are too! So we should look a the other columns with non-null values to see if they could have errors that we can correct. Here, the other columns are `business_id`, `review_id`, `user_id`, and `text`. However, IDs are arbitrary identifiers which we have no way of ascertaining if they are correct or not, so we have to take the values we are given for granted.\n",
    "\n",
    "`text` is the text of the user review, but the objective of any attempts to clean this is unclear - what metrics would we use to determine if a review text is \"clean\" or not? The answer to this question is not obvious at all without much more precise direction from the client about what they intend to use this text for, so we will leave it alone for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up `businesses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking which columns contain null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business_id    False\n",
       "categories      True\n",
       "city           False\n",
       "latitude       False\n",
       "longitude      False\n",
       "name           False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "businesses.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are null values in `categories`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Suppose for a moment that there were missing values in the `city` column. Can you describe a method which would allow us to effectively fill in missing `city` values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Describe and implement the best method to fill in the null values in the `categories` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another look at erroneous values\n",
    "\n",
    "Of course, missing values are not the only thing that can go wrong in a dataset – erroneous values can affect the analysis as well. Looking at the columns which we have not dealt with yet, we have `business_id`, `latitude`, `longitude`, and `name` as potential candidates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Describe and implement suitable methods to deal with potential erroneous values in these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding information for `businesses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the client's request, we must find the county and state each business belongs in and add this to the table. We use the geoJSON files that contain the geoshapes of each state and county in the US. We will be using the [shapely](https://shapely.readthedocs.io/en/latest/manual.html) library to figure out whether a point is present inside the shape that represents the border of each state/county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('us-state-shapes.json') as f:\n",
    "    states = json.load(f)\n",
    "    \n",
    "def get_state_name(row):\n",
    "    if not row['latitude'] or not row['longitude']:\n",
    "        return None\n",
    "    point = Point(row['longitude'], row['latitude'])\n",
    "    for state in states['features']:\n",
    "        polygon = shape(state['geometry'])\n",
    "        if polygon.contains(point):\n",
    "            return state['properties']['NAME']\n",
    "        \n",
    "businesses['state'] = businesses.apply(get_state_name, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Find the county of each business and add them to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to add information about a restaurant's number of reviews and their average star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Calculate the number of reviews received for each restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8:\n",
    "\n",
    "For each restaurant, calculate its avearge star rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating the check-ins data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do our final task: use the check-ins data given in the JSON file to compute various statistics on the number oof check-ins over various time periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9:\n",
    "\n",
    "Compute, for each business, the total number of check-ins by hour of day, by weekday, by day, and across all time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy notebook and data files to EC2 instance - Take Home Task\n",
    "\n",
    "We have performed these operations only on the first 10000 rows of our data tables. We will set up a Jupyter Notebook in an EC2 instance to apply these operations to the entire dataset. We need to open port 8889 on the EC2 instance.\n",
    "\n",
    "Now, the process will not take as long this time, because we have already done most of the setup in the previous case. Let's copy the notebook file and the data files from the local machine to the EC2 instance. It can be done via an scp command, or using winSCP. There are different ways to SCP (secure copy) the file into the server:\n",
    "\n",
    "1. If you are using a Linux or a Mac, type the following command to copy the file:\n",
    "\n",
    "```\n",
    "scp -i <path_to_pem_file> </path/to/jupyter/notebook> <username>@<EC2IP>:</destination/path>\n",
    "```\n",
    "\n",
    "2. If you are using Windows, you can either use WinSCP - https://winscp.net/eng/index.php or if you are using Putty, you can use the pscp command:\n",
    "\n",
    "```\n",
    "pscp -i <path_to_ppk_file> <\\path\\to\\jupyter\\notebook\\> <username>@<EC2IP>:</destination/path>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the entire set with full data in the server - Take Home Task\n",
    "\n",
    "Once the file is copied, you can now access the jupyter notebook from `http:server_ip:8889`. Once you are able to access the file, just remove the `nrows=10000` parameter from the [read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function call, in order to read the entire dataset. You can now click on `Cells > Run All` in order to run the same set of steps on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this case, we practiced our skills in dealing with missing values and also learned about common ways of handling erroneous values. We then creatively leveraged external data in order to engineer additional features based on client requests. We again took a small local sample of the overall data, then uploaded our work to Amazon EC2 so that they could be applied to the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "In this case, you looked more at how to deal with missing data. You learned that although interpolation is a powerful tool, when and how to use it depends highly on the projected use cases of the data. Sometimes, it is better to simply replace a missing value with \"Not found\" or `NaN` so that it is clear to end users to discount it appropriately from subsequent analyses, rather than run the risk of skewing the resultant data distribution or summary statistics.\n",
    "\n",
    "You also learned a few ways of using existing public data to help interpolate missing values in your current dataset. This is an invaluable skill and often times the best way to deal with missing values is to exercise this resourcefulness. It is important to recognize not all interpolation is made equal. If you have many parameters that are imputed poorly or with great uncertainty, it will be much harder to develop an accurate prediction model later if those parameters are key predictors.\n",
    "\n",
    "Finally, you looked at erroneous values and various common ways these could leak into the data. You also learned that sometimes it is too difficult or impossible to systematically determine existence of error.\n",
    "    \n",
    "We dove into the technical nuances of data cleaning in this case but it is important to note this dataset is very simple compared to the data you will be working with in real life. It is not too uncommon to deal with datasets with hundreds of parameters where dozens have different types of missing/incorrect data. Additionally, you may encounter datasets where you are unsure what some columns/parameters mean. This dataset came with a nice handy dictionary explaining what information is in each column. \n",
    "    \n",
    "Students are highly encouraged to thoroughly review all the EDA-related cases we have taught up to this point. Mastery of EDA will immensely improve the quality of your data wrangling & cleaning process, which in combination will impact your subsequent modeling process in a positive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
