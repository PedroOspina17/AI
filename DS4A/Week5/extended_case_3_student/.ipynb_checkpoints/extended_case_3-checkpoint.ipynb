{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding our best-performing salespeople and products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context.** You work for AdventureWorks, a company that sells outdoor sporting equipment. The company has many different locations and has been recording the sales of different locations on various products. You, their new data scientist, have been tasked with the question: **\"What are our best products and salespeople and how can use this information to improve our overall performance?\"**\n",
    "\n",
    "You have been given access to the relevant data files with documentation from the IT department. Your job is to extract meaningful insights from these data files to help increase sales. First, you will look at the best products and try to see how different products behave in different categories. Second, you will analyze the best salespeople to see if the commission percentage motivates them to sell more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem.** Your task is to **construct a database from the provided CSV files and then write queries in SQL to carry out the requested analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Context.** You are given the data (stored in the ```data/csvs``` folder) as a set of separate CSV files, each one representing a table. You will build a new PostgreSQL database from these files using AWS RDS.\n",
    "\n",
    "The company has been pretty vague about how they expect you to extract insights, but you have come up with the following plan of attack:\n",
    "\n",
    "1. Create the database and ensure you can run basic queries against it\n",
    "2. Look at how product ratings and total sales are related\n",
    "3. See how products sell in different subcategories (bikes, helmets, socks, etc.)\n",
    "4. Calculate which salespeople have performed the best in the past year\n",
    "5. Seeing if total sales are correlated with their commission percentage\n",
    "\n",
    "Of course, this is only your initial plan. As you explore the database, your strategy will change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we'll assume that the company has given you an entry-level laptop, which is not capable of running a PostgreSQL server locally. Therefore, you should set up a cloud database, connect to it from `psql`, and run the analysis via the `psql` or directly from the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Repeat the steps in Case 12.3 to create a new RDS instance with a PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for the case is contained in the ```./data/csvs``` directory; specifically, it is the ```AdventureWorks``` sample data provided by Microsoft. We will be focusing on the Sales and Production categories. Complete documentation for the original data (of which you have only a subset) can be found [here](https://dataedo.com/download/AdventureWorks.pdf). \n",
    "\n",
    "**Product Tables:**\n",
    "* **Product**: one row per product that the company sells\n",
    "* **ProductReview**: one row per rating and review left by customers\n",
    "* **ProductModelProductDescriptionCulture**: a link between products and their longer descriptions also indicating a \"culture\" - which language and region the product is for\n",
    "* **ProductDescription**: a longer description of each product, for a specific region\n",
    "* **ProductCategory**: the broad categories that products fit into\n",
    "* **ProductSubCategory**: the narrower subcategories that products fit into\n",
    "\n",
    "**Sales Tables:**\n",
    "* **SalesPerson**: one row per salesperson, including information on their commission and performance\n",
    "* **SalesOrderHeader**: one row per sale summarizing the sale\n",
    "* **SalesOrderDetail**: many rows per sale, detailing each product that forms part of the sale\n",
    "* **SalesTerritory**: the different territories where products are sold, including performance\n",
    "\n",
    "**Region Tables:**\n",
    "* **CountryRegionCurrency**: the currency used by each region\n",
    "* **CurrencyRate**: the average and closing exchange rates for each currency compared to the USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up `ipython-sql` and `pgspecial`\n",
    "\n",
    "Jupyter notebook is usually used to run Python code, but with an add-on it can run SQL directly against a database too. Install the extensions `ipython-sql` and `pgspecial` through `pip` (you may have to restart the notebook after doing this) and create the database `adventureworks`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-sql\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/3d/0d38357c620df31cebb056ca1804027112e5c008f4c2c0e16d879996ad9f/ipython_sql-0.4.0-py3-none-any.whl\n",
      "Collecting pgspecial\n",
      "  Downloading https://files.pythonhosted.org/packages/34/70/f20df1e335592ace0e4f54989307d1630163445599b6abd43a2e0149d483/pgspecial-1.11.10-py3-none-any.whl\n",
      "Collecting sqlparse\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/ee/6e821932f413a5c4b76be9c5936e313e4fc626b33f16e027866e1d60f588/sqlparse-0.3.1-py2.py3-none-any.whl (40kB)\n",
      "\u001b[K     |████████████████████████████████| 40kB 7.8MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython>=1.0 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython-sql) (6.5.0)\n",
      "Requirement already satisfied: sqlalchemy>=0.6.7 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython-sql) (1.2.11)\n",
      "Collecting prettytable<1\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/30/4b0746848746ed5941f052479e7c23d2b56d174b82f4fd34a25e389831f5/prettytable-0.7.2.tar.bz2\n",
      "Requirement already satisfied: six in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython-sql) (1.11.0)\n",
      "Requirement already satisfied: ipython-genutils>=0.1.0 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython-sql) (0.2.0)\n",
      "Requirement already satisfied: click>=4.1 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from pgspecial) (6.7)\n",
      "Requirement already satisfied: psycopg2>=2.7.4 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from pgspecial) (2.8.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.8.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (4.6.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (1.0.15)\n",
      "Requirement already satisfied: decorator in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (4.3.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (4.3.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.12.1)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (42.0.0)\n",
      "Requirement already satisfied: backcall in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.1.0)\n",
      "Requirement already satisfied: pygments in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (2.2.0)\n",
      "Requirement already satisfied: pickleshare in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from ipython>=1.0->ipython-sql) (0.7.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=1.0->ipython-sql) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->ipython>=1.0->ipython-sql) (0.1.7)\n",
      "Requirement already satisfied: parso>=0.3.0 in /Users/tongzhan/anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython>=1.0->ipython-sql) (0.3.1)\n",
      "Building wheels for collected packages: prettytable\n",
      "  Building wheel for prettytable (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for prettytable: filename=prettytable-0.7.2-cp37-none-any.whl size=12666 sha256=11d0421628ecd487428af89f1afc1e203c3fec52600c2897c88c39afb58106b0\n",
      "  Stored in directory: /Users/tongzhan/Library/Caches/pip/wheels/80/34/1c/3967380d9676d162cb59513bd9dc862d0584e045a162095606\n",
      "Successfully built prettytable\n",
      "Installing collected packages: sqlparse, prettytable, ipython-sql, pgspecial\n",
      "Successfully installed ipython-sql-0.4.0 pgspecial-1.11.10 prettytable-0.7.2 sqlparse-0.3.1\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ipython-sql pgspecial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the sql add-on and connect to the database as follows. You'll need to change the username (`postgres`), password (`mysecretpassword`), host (`localhost`), and database name (`postgres`) to what you used when setting up your RDS instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql postgresql://postgres:mysecretpassword@localhost/postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to run SQL directly from any Jupyter notebook cell by starting the cell with a line that states `%%sql`. For example (once you have a database with some tables, which we'll only create later):\n",
    "\n",
    "```sql\n",
    "%%sql\n",
    "\n",
    "SELECT * FROM product LIMIT 10;\n",
    "```\n",
    "\n",
    "**Note:** Unlike `pandas` which automatically truncates output for large DataFrames, the SQL plug-in gives you exactly what you ask for. If you do a `SELECT * FROM` a table with a million rows and no `LIMIT` clause, it'll output all million rows and probably freeze your notebook. It's good practice to always use a `LIMIT` clause even when it's not needed to avoid any mishaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the database and adding the tables\n",
    "\n",
    "Now, let's create a database called `adventuretime`. (If you do this through the notebook, you'll have to add the line `end;` before your `create database` command as the add-on runs everything in transactions).\n",
    "\n",
    "You'll need to add a table for each of the CSV files. Spend some time looking at the different CSV files and getting used to how they reference each other and what headers they create. Then, you'll need to write an appropriate `CREATE TABLE` command with appropriate types. You can figure out the types by inspecting the CSV files and/or referencing the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "Write all of the commands that you need to\n",
    "\n",
    "* Create the database\n",
    "* Create the tables\n",
    "* Import the data from the CSVs\n",
    "\n",
    "**Hint:** As an example, to add data for the `salesperson` table, you would use the following commands:\n",
    "\n",
    "1. Create table (can be run from Jupyter Notebook or the `psql` command line interface):\n",
    "```sql\n",
    "CREATE TABLE salesperson (\n",
    "    businessentityid INTEGER,\n",
    "    territoryid INTEGER,\n",
    "    salesquota INTEGER,\n",
    "    bonus INTEGER,\n",
    "    commissionpct FLOAT,\n",
    "    salesytd FLOAT,\n",
    "    saleslastyear FLOAT,\n",
    "    rowguid TEXT,\n",
    "    modifieddate DATE\n",
    "    );\n",
    "```\n",
    "\n",
    "2. Copy data (has to be run from the `psql` shell):\n",
    "\n",
    "```sql\n",
    "\\copy salesperson FROM 'data/csvs/salesperson.csv' with (format CSV, header true, delimiter ',');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding our most popular products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, the company would like to know which of their products is the most popular among customers. You figure that the average rating given in reviews is correlated with the number of sales of a particular product (that products with higher reviews have more sales)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Using the ```product``` and ```productreview``` tables, ```JOIN``` them and rank the products according to their average review rating. What are the names and IDs of the top 5 products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Much to your disappointment, there are only three products with ratings and only four reviews in total! This is nowhere near enough to perform an analysis of the correlation between reviews and total sales.\n",
    "\n",
    "Nevertheless, your manager wants the **English description** of these products for an upcoming sale. Use the documentation provided above if you need help navigating the structure to extract this!\n",
    "\n",
    "**Hint:** You'll notice that the value for `cultureid` in the `productmodelproductdescriptionculture` table often has extra trailing spaces which makes it difficult to reliably get descriptions of a specific language. You should first modify this table before writing the `SELECT` statement to get the descriptions that your manager wants. To do this, you can use an `UPDATE` statement with Postgres's [`TRIM`](https://w3resource.com/PostgreSQL/trim-function.php) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Since we cannot infer the most popular products from the reviews, we will go with an alternative strategy.\n",
    "\n",
    "Get the model ID, name, description, and total number of sales for each product and display the top-10 selling products. You can infer how often products have been sold by looking at the `salesorderdetail` table (each row might indicate more than one sale, so take note of `OrderQty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Let's look at the correlation between quantity sold and price for each item in each subcategory. Some subcategories don't have enough sales to make the correlation meaningful, so only look at the top 10 subcategories by total quantity of sales.\n",
    "\n",
    "Once you've looked at the data, make a hypothesis about what causes any positive or negative correlations between price and quantity, and explain this in 2-3 sentences.\n",
    "\n",
    "**Hint:** You'll need to calculate the total quantities from `salesorderdetail` again and group the products by subcategory. It'll probably be easier if you use at least two [CTEs](https://www.postgresql.org/docs/9.1/queries-with.html). You can calculate the correlation in PostgreSQL by using the built-in [```corr()```](https://www.postgresql.org/docs/9.4/functions-aggregate.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding our top salespeople\n",
    "\n",
    "As mentioned earlier, we want to find our best salespeople and see whether or not we can incentivize them in an appropriate manner. Namely, we want to determine if the commission percentage we give them motivates them to make more and bigger sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Find the top five performing salespeople by using the `salesytd` (Sales, year-to-date) column. (We only need to know the `businessentityid` for each salesperson as this uniquely identifies each.) Why might you be skeptical of these numbers right now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Using ```salesorderheader```, find the top 5 salespeople who made the most sales **in the most recent year** (2014). (There is a column called `subtotal` - use that.) Sales that do not have an associated salesperson should be excluded from your calculations and final output. All orders that were made within the 2014 calendar year should be included.\n",
    "\n",
    "**Hint:** You can use the syntax `'1970-01-01'::date` to generate an arbitrary date in PostgreSQL and compare this to specific dates in the tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Looking at the documentation, you will see that `subtotal` in the ```salesorderheader``` table is calculated from other tables in the database. To validate this figure (instead of trusting it blindly), let's calculate `subtotal` manually. Using the ```salesorderdetail``` and ```salesorderheader``` tables, calculate the sales for each salesperson for **this past year** (2014) and display results for the top 5 salespeople.\n",
    "\n",
    "**Hint:** You will have to ```JOIN``` ```salesorderdetail``` on ```salesorderheader``` to get the salesperson, calculate line totals for each sale using appropriate discounts, then sum all the line totals to get the total sale. You will want to use ```WITH``` clauses again to keep things sane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8:\n",
    "\n",
    "Using ```corr()```, see if there is a positive relationship between total sales and commission percentage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9:\n",
    "\n",
    "Remember how we mentioned that products were sold in many regions? This is why you had to work with the `culture` value before to get the English language descriptions. To make matters worse, you are told the sales are recorded in **local** currency, so your previous analysis is flawed, and you must convert all amounts to USD if you wish to compare the different salespeople fairly!\n",
    "\n",
    "Use the `countryregioncurrency` table in combination with the `salesperson` and `salesterritory` ones to figure out the relevant currency symbol for each of the top salespeople."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10:\n",
    "\n",
    "Now that we have the currency codes associated with each salesperson, redo Exercise 7 to take the currency exchange into account. If there are salespeople in the top 5 that weren't there before, explain why.\n",
    "\n",
    "**Hint:** The rates in the```currencyrate``` table always go from `FromCurrencyCode=USD` to `ToCurrencyCode=<Desired Currency Code>`, and they are listed every day. When calculating line totals, use the `AverageRate` for that day. You should be able to reuse a lot of Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11:\n",
    "\n",
    "How does the correlation from Exercise 8 change once you've adjusted for the currency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
