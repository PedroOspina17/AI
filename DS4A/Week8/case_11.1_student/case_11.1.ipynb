{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can I compare different models that predict the probability of defaulting on a loan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: (3 min)\n",
    "\n",
    "In this case, we will conduct a comprehensive walkthrough of a binary classification data science problem. We aim to combine the various pre-modeling techniques we hae learned with **cross-validation** to tune a logistic regression model.\n",
    "\n",
    "Additionally, we introduce various metrics (ROC/AUC) to evaluate the performance of our classification model which is inherently different from the previous linear regression models we have encountered. This goal may seem a bit daunting or technical but by the end of the case, all these terms should be clear to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load relevant packages\n",
    "import pandas                  as pd\n",
    "import numpy                   as np\n",
    "import matplotlib.pyplot       as plt\n",
    "import seaborn                 as sns\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction (5 min)\n",
    "\n",
    "**Business Context.** Traditional commercial banks typically did not rely on statistical modeling to decide whether personal loans should be issued, although this is changing rapidly nowadays. You are a data scientist working in a modern commercial bank. Your data science team has already built simple regression models for predicting the probability that those loans would be defaulted on. However, you have noticed that many of these models perform much worse in production than they do in testing.\n",
    "\n",
    "**Business Problem.** Your task is to **build a default probability model that you feel comfortable putting into production**.\n",
    "\n",
    "**Analytical Context.** The dataset contains the details of 5000 loan requests that have been previously issued by your bank. For each loan, the final status of the loan (i.e. whether the loan defaulted) is also available:\n",
    "\n",
    "1. The file **\"loan_light.csv\"** contains the details of 5000 loans\n",
    "2. The file **\"loan_param.xlsx\"** contains the description of each covariate\n",
    "\n",
    "The case will proceed as follows: you will 1) perform some data exploration to determine the appropriate variable transformations to make; 2) fit some simple models; 3) learn about **cross-validation** and use this to select the best simple model; and finally 4) responsibly construct more complex models using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration (40 min)\n",
    "\n",
    "Let's start by taking a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"loan_light.csv\")\n",
    "Data = Data.sample(frac=1)  #shuffle the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>application_type</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>dti</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>grade</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>installment</th>\n",
       "      <th>...</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>term</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>purpose</th>\n",
       "      <th>year</th>\n",
       "      <th>loan_default</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>22624.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.17</td>\n",
       "      <td>8</td>\n",
       "      <td>B</td>\n",
       "      <td>4.0</td>\n",
       "      <td>560.52</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>underwriter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>46000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>5354.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.50</td>\n",
       "      <td>10</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>294.61</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3673</th>\n",
       "      <td>45000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>10583.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.08</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>431.15</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4546</th>\n",
       "      <td>100000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>33157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.08</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>530.22</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>60</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>90000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>5040.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.76</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1.0</td>\n",
       "      <td>497.96</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>advisor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      annual_inc application_type  avg_cur_bal  chargeoff_within_12_mths  \\\n",
       "4770    100000.0       Individual      22624.0                       0.0   \n",
       "2021     46000.0       Individual       5354.0                       0.0   \n",
       "3673     45000.0       Individual      10583.0                       0.0   \n",
       "4546    100000.0       Individual      33157.0                       0.0   \n",
       "3705     90000.0       Individual       5040.0                       0.0   \n",
       "\n",
       "      delinq_2yrs    dti  emp_length grade  inq_last_12m  installment  ...  \\\n",
       "4770          0.0  12.17           8     B           4.0       560.52  ...   \n",
       "2021          0.0  15.50          10     B           0.0       294.61  ...   \n",
       "3673          0.0  12.08           1     C           1.0       431.15  ...   \n",
       "4546          0.0  19.08           5     C           0.0       530.22  ...   \n",
       "3705          0.0  20.76           1     D           1.0       497.96  ...   \n",
       "\n",
       "      num_actv_bc_tl  pub_rec_bankruptcies  home_ownership term  mort_acc  \\\n",
       "4770             3.0                   0.0        MORTGAGE   36       2.0   \n",
       "2021             3.0                   0.0        MORTGAGE   36       0.0   \n",
       "3673             3.0                   0.0        MORTGAGE   60       1.0   \n",
       "4546             2.0                   0.0        MORTGAGE   60       2.0   \n",
       "3705             4.0                   1.0            RENT   60       0.0   \n",
       "\n",
       "      num_tl_90g_dpd_24m             purpose  year  loan_default          job  \n",
       "4770                 0.0  debt_consolidation  2017             0  underwriter  \n",
       "2021                 0.0         credit_card  2016             0        other  \n",
       "3673                 0.0  debt_consolidation  2016             1      teacher  \n",
       "4546                 0.0  debt_consolidation  2016             0        other  \n",
       "3705                 0.0  debt_consolidation  2016             0      advisor  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['annual_inc', 'application_type', 'avg_cur_bal',\n",
       "       'chargeoff_within_12_mths', 'delinq_2yrs', 'dti', 'emp_length', 'grade',\n",
       "       'inq_last_12m', 'installment', 'loan_amnt', 'num_actv_bc_tl',\n",
       "       'pub_rec_bankruptcies', 'home_ownership', 'term', 'mort_acc',\n",
       "       'num_tl_90g_dpd_24m', 'purpose', 'year', 'loan_default', 'job'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow0_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow1_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow2_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow3_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow4_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow5_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow6_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow7_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow8_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow9_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow10_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow11_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow12_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow13_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow14_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow15_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow16_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow17_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow18_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow19_col1 {\n",
       "            width:  1000px;\n",
       "        }    #T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow20_col1 {\n",
       "            width:  1000px;\n",
       "        }</style><table id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3fe\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >BrowseNotesFile</th>        <th class=\"col_heading level0 col1\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow0_col0\" class=\"data row0 col0\" >loanAmnt</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow0_col1\" class=\"data row0 col1\" >The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow1_col0\" class=\"data row1 col0\" >annualInc</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow1_col1\" class=\"data row1 col1\" >The self-reported annual income provided by the borrower during registration.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow2_col0\" class=\"data row2 col0\" >application_type</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow2_col1\" class=\"data row2 col1\" >Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow3_col0\" class=\"data row3 col0\" >avg_cur_bal</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow3_col1\" class=\"data row3 col1\" >Average current balance of all accounts</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow4_col0\" class=\"data row4 col0\" >chargeoff_within_12_mths</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow4_col1\" class=\"data row4 col1\" >Number of charge-offs within 12 months</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow5_col0\" class=\"data row5 col0\" >delinq2Yrs</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow5_col1\" class=\"data row5 col1\" >The number of 30+ days past-due incidences of delinquency in the borrower's credit file for the past 2 years</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow6_col0\" class=\"data row6 col0\" >dti</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow6_col1\" class=\"data row6 col1\" >A ratio calculated using the borrowerâ€™s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrowerâ€™s self-reported monthly income.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow7_col0\" class=\"data row7 col0\" >emp_length</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow7_col1\" class=\"data row7 col1\" >Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow8_col0\" class=\"data row8 col0\" >grade</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow8_col1\" class=\"data row8 col1\" >LC assigned loan grade</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow9_col0\" class=\"data row9 col0\" >homeOwnership</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow9_col1\" class=\"data row9 col1\" >The home ownership status provided by the borrower during registration. Our values are: RENT, OWN, MORTGAGE, OTHER.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow10_col0\" class=\"data row10 col0\" >inq_last_12m</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow10_col1\" class=\"data row10 col1\" >Number of credit inquiries in past 12 months</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow11_col0\" class=\"data row11 col0\" >installment</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow11_col1\" class=\"data row11 col1\" >The monthly payment owed by the borrower if the loan originates.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow12_col0\" class=\"data row12 col0\" >job</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow12_col1\" class=\"data row12 col1\" >Job Description</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow13_col0\" class=\"data row13 col0\" >loanAmnt</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow13_col1\" class=\"data row13 col1\" >The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow14_col0\" class=\"data row14 col0\" >loanDefault</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow14_col1\" class=\"data row14 col1\" >0: Loan was uptimated paid in full. 1: A default even occurred</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow15_col0\" class=\"data row15 col0\" >mortAcc</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow15_col1\" class=\"data row15 col1\" >Number of mortgage accounts.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow16_col0\" class=\"data row16 col0\" >num_tl_90g_dpd_24m</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow16_col1\" class=\"data row16 col1\" >Number of accounts 90 or more days past due in last 24 months</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow17_col0\" class=\"data row17 col0\" >pub_rec_bankruptcies</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow17_col1\" class=\"data row17 col1\" >Number of public record bankruptcies</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow18_col0\" class=\"data row18 col0\" >purpose</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow18_col1\" class=\"data row18 col1\" >A category provided by the borrower for the loan request. </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow19_col0\" class=\"data row19 col0\" >term</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow19_col1\" class=\"data row19 col1\" >The number of payments on the loan. Values are in months and can be either 36 or 60.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3felevel0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow20_col0\" class=\"data row20 col0\" >Year</td>\n",
       "                        <td id=\"T_302f1d04_7dbb_11ea_a1a5_f218981cf3ferow20_col1\" class=\"data row20 col1\" >Year of Issue of the loan</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x12fa818d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_description = pd.read_excel('loan_param.xlsx').dropna()\n",
    "df_description.style.set_properties(subset=['Description'], **{'width': '1000px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: (20 min)\n",
    "\n",
    "For each of the following, perform the directed visualization and discuss your conclusions from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 (4 min)\n",
    "\n",
    "Create a [bar chart](https://www.mathsisfun.com/data/bar-graphs.html) showing the number of loans that did and did not default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='loan_default', data = Data)\n",
    "plt.title(\"Loan Default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that around 20 - 25 percent of all loans in the dataset defaulted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 (3 min)\n",
    "\n",
    "Plot a [histogram](https://www.mathsisfun.com/data/histograms.html) of the annual incomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.annual_inc.hist(bins=100, density=True)\n",
    "plt.title(\"Annual Income Distribution\")\n",
    "plt.xlabel(\"Annual Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is quite skewed. Let's try a [logarithmic transformation](https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c) to get the data to be more normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(Data.annual_inc).hist(bins=100, density=True)\n",
    "plt.title(\"Annual Income Distribution\")\n",
    "plt.xlabel(\"Annual Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 (5 min)\n",
    "\n",
    "Is the distribution of annual incomes different between applicants who defaulted vs. applicants who did not default on their loans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(Data['annual_inc'][Data.loan_default == 0]).hist(bins=50, density=True, alpha=0.5, label=\"Paid\")\n",
    "np.log(Data['annual_inc'][Data.loan_default == 1]).hist(bins=50, density=True, alpha=0.5, label=\"Default\")\n",
    "plt.xlabel(\"Annual Income\")\n",
    "plt.legend()\n",
    "plt.title(\"Annual Salary -- Loan Status\", fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distributions are not that different, indicating that income alone is not likely to explain a significant fraction of the difference in loan default status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 (8 min)\n",
    "\n",
    "Explore the association between annual income and the monthly installment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(Data.installment, Data.annual_inc, kind=\"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the distributions for both variables are skewed in similar ways. This means both ought to undergo a log transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(np.log(Data.installment), np.log(Data.annual_inc), kind=\"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better and seems to suggest (with some outliers on the far left) a linear relationship between the logarithms of both variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more figures which look at the relationship between other numerical covariates and the probability of default, as well as annual income:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emp_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize = (10,5), ncols=2, sharey= False)\n",
    "sns.boxplot(x='emp_length', y = 'annual_inc', data = Data, showfliers=False, ax = ax1) #showfliers=False for nice display\n",
    "ax1.set_title(\"Employment length vs annual income\")\n",
    "Data[[\"emp_length\",'loan_default']].groupby(\"emp_length\").mean().plot.bar(rot=90,ax = ax2)\n",
    "plt.title(\"Default probability vs. home ownership\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`homeOwnership`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize = (10,5), ncols=2, sharey= False)\n",
    "sns.boxplot(x=\"home_ownership\",y=\"annual_inc\", data = Data, showfliers=False, ax = ax1) #showfliers=False for nice display\n",
    "ax1.set_title(\"Home ownership vs. annual income\")\n",
    "Data[[\"home_ownership\",'loan_default']].groupby(\"home_ownership\").mean().plot.bar(rot=90,ax = ax2)\n",
    "plt.title(\"Default probability vs. home ownership\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some figures that show the relationship between various categorical variables and the probability of default:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`purpose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "Data.emp_length.value_counts()\n",
    "sns.countplot(x='purpose', order=Data['purpose'].value_counts().index, data = Data) \n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribution of Loan Purposes\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "purpose_default = Data[[\"loan_default\", \"purpose\"]].groupby(\"purpose\").mean()\n",
    "purpose_default = purpose_default.sort_values(by=\"loan_default\",axis=0, ascending=False)\n",
    "sns.barplot(x=purpose_default.index[:30], \n",
    "            y=purpose_default[\"loan_default\"][:30].values,\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Default Probability\");\n",
    "plt.title(\"Default Probability by Loan Purpose\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`job`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15,5))\n",
    "sns.barplot(x=Data[\"job\"].value_counts()[:30].index.values , \n",
    "            y=100 * Data.job.value_counts()[:30].values / len(Data),\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Percentage of Population\")\n",
    "plt.title(\"Distribution of Jobs\", fontsize=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "\n",
    "df_job_default = Data[[\"loan_default\", \"job\"]].groupby(\"job\").mean()\n",
    "df_job_default = df_job_default.sort_values(by=\"loan_default\",axis=0, ascending=False)\n",
    "sns.barplot(x=df_job_default.index[:50], \n",
    "            y=df_job_default[\"loan_default\"][:50].values,\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.ylabel(\"Defaut Probability\")\n",
    "plt.title(\"Default Probability by Job Type\", fontsize=20, verticalalignment='bottom');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new variable\n",
    "\n",
    "The yearly payment owed by the borrower, as a fraction of their annual income, is a standard metric used in evaluating whether a loan should be issued. Let's define a new variable **\"install_income\"** which codes the installment as a fraction of the annual income and study its association with the other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['install_income'] = 12 * Data.installment / Data.annual_inc\n",
    "H = plt.hist(Data['install_income'], bins=100, density=True)\n",
    "plt.xlabel(r\"Installment / Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily investigate this variable's association with the probability of default, define a new covariate named `install_income_disc` that is a discretized version of `install_income`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us discretize the \"install_income\" variable to study the probability of default \n",
    "# as a function of \"install_income\"\n",
    "Data[\"install_income_disc\"] = (Data.install_income*50).astype(int)/50.  #discretization\n",
    "Data[[\"loan_default\", \"install_income_disc\"]].groupby(\"install_income_disc\").mean().plot.bar(rot=90)\n",
    "Data = Data.drop([\"install_income_disc\"], axis=1)\n",
    "\n",
    "# --> there is a clear positive association: as the fraction of the annual income devoted to the re-imbursement of \n",
    "# the loan increases, the probability of default sharply increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: (10 min)\n",
    "\n",
    "Visualize the [correlation matrix](https://www.statisticshowto.com/correlation-matrix/) across all numerical features by using the `sns.heatmap()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute correlation matrix\n",
    "df_correlations = Data.corr()\n",
    "\n",
    "#mask the upper half for visualization purposes\n",
    "mask = np.zeros_like(df_correlations, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "plt.figure(figsize= (10,10))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(df_correlations,mask=mask,  vmax=1, vmin=-1, cmap=cmap, \n",
    "            center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that `installment` and `loan_amnt` are highly correlated. In general, we can use a correlation matrix as an early feature selection technique to remove features that are extremely correlated to account for multicollinearity. There is no scientific cutoff for when a correlation warrants removing a feature so always investigate further. Think of it as a red flag moment when you see highly correlated parameters and then investigate the reason behind such a correlation to see if one of the features can be removed. A classic example of correlated features that require removal would be two features displaying distance in miles and kilometers, respectively. There is clearly redundant information there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a predictive model (20 min)\n",
    "\n",
    "Let's first start by building a standard [logistic regression model](https://www.youtube.com/watch?v=yIYKR4sgzI8). In general, it is important and extremely useful to first create baseline/simple models which can be compared to more complex models later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: (15 min)\n",
    "\n",
    "#### 3.1 (5 min)\n",
    "\n",
    "Using the `LogisticRegression()` function from `scikit-learn`, write a function named `fit_logistic_regression(X,y)` that fits a logistic regression on the array of covariates `X` and associated response variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def fit_logistic_regression(X,y):\n",
    "    \"\"\"\n",
    "    fit a logistic regression with feature matrix X and binary output y\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(solver='lbfgs', tol=10**-4,  \n",
    "                             fit_intercept=True, \n",
    "                             multi_class='multinomial').fit(X,y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 (5 min)\n",
    "\n",
    "Create a basic [logistic regression model](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) for predicting the loan default with only one feature: `install_income`.  Call this model `model1`. Use a 70/30 train-test split of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use a 70%/30% split for training/validation\n",
    "n_total = len(Data)\n",
    "n_train = int(0.7*n_total)\n",
    "\n",
    "X, y = Data[[\"install_income\"]], Data.loan_default\n",
    "X_train, y_train = X[:n_train], y[:n_train]\n",
    "X_test, y_test = X[n_train:], y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = fit_logistic_regression(X_train, y_train)  # fit a logistic regression\n",
    "y_test_pred = model1.predict_proba(X_test)[:,1]     # make probabilistic predictions on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted a model, we will proceed to evaluate how \"good\" it is. Classification models are judged differently than linear regression models. A common tool you will have to get used to is the [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62). Additionally, we will introduce some other metrics related to classification algorithms below. It is important to note these metrics apply to other classification models, not just logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 (5 min)\n",
    "\n",
    "Plot the [ROC curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) of `model1` and find the area under the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_test_pred)  #compute FPR/TPR\n",
    "auc_baseline = auc(fpr, tpr) # compute AUC\n",
    "\n",
    "plt.plot(fpr, tpr, \"b-\", label=\"AUC(basline)={:2.2f}\".format(auc_baseline))\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(fontsize=15)\n",
    "plt.plot([0,1], [0,1], \"r--\")\n",
    "plt.title(\"ROC curve -- Baseline Model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: (5 min)\n",
    "\n",
    "#### 4.1 (2 min)\n",
    "\n",
    "Consider `model1` from above. Would you want this to be your final model? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** This should not be the final model. This is because we have not explored the contribution from other variables, which in addition to containing valuable information could also be confounding the perceived effect of `install_income` on the response variable. This under-exploitation of information is called [**underfitting**](https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 (3 min)\n",
    "\n",
    "Let's instead put all the variables available in the model, so that we are maximally leveraging our available info. Would you be in favor of this or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** This is also a bad idea. If we *blindly* use all of the variables in our model fitting, a phenomenon called [**overfitting**](https://towardsdatascience.com/what-are-overfitting-and-underfitting-in-machine-learning-a96b30864690) occurs. This is when a statistical model \"fits\" too closely to a particular set of data, which may well be noisy and exhibit randomness and therefore fail to predict future, different observations reliably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, you will be working with datasets with many features that each have their own distribution. Generally, a large amount of time is spent on feature selection with many models being trained during this time. It is extremely rare that you simply plug all the features in and tune it once to get the optimal model. \n",
    "    \n",
    "There are many different techniques associated with feature selection and a comprehensive look into all of them is outside the scope of this case. For simplicity, we will demonstrate model training and testing on single-feature models and then directly move into multi-feature models to show the numerous possible cases you may encounter. In reality, we would apply cross-validation on numerous subsets of features based on domain knowledge of the dataset to see which set of features truly optimizes the model we are trying to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation (30 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Cross-validation**](https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f) is a set of techniques for assessing how well the results of a model will generalize to an out-of-sample dataset; i.e. in practice or production. It is chiefly used to flag overfitting.\n",
    "\n",
    "Cross-validation works as follows: one splits the available data into $k$ sets, or **folds**. $k - 1$ of these folds will be used to train the model, while the held-out fold will be used as the test set on which the model is evaluated. For computational stability, this procedure is generally split many times, such that each fold has an opportunity to serve as the test set. For each repetition, a metric of prediction performance (e.g. AUC) is calculated on the test set. The average of these metrics, as well as their standard deviation, is then reported. \n",
    "\n",
    "There is no exact science for choosing a proper $k$ for your dataset. It depends on what type of data you are using and how large it is. The larger $k$ is, the most iterations you run which lowers the chance you get \"unlucky\"; however, running more iterations can be costly in some cases. Additionally, it is important to make sure no matter what $k$ you choose that the validation set is still large enough to be meaningful. If you have a data set of 100 points and choose a $k$ of 20, then the validation set will only be 5 points. It is likely some iterations will perform poorly simply because the validation set is too small and potentially too different from the training set.\n",
    "\n",
    "An example is shown here for 5-fold cross-validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cv_fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this with code. The following code displays the 5 different folds used in a standard 5-fold cross-validation approach. To do so, use the `StratifiedKFold()` function from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "for k, (train_index, test_index) in enumerate( skf.split(X, y) ):\n",
    "    plt.plot(train_index, [k+1 for _ in train_index], \".\")\n",
    "plt.ylim(0,6)\n",
    "plt.ylabel(\"FOLD\")\n",
    "plt.title(\"CROSS VALIDATION FOLDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a function `compute_AUC(X, y, train_index, test_index)` that computes the AUC of a model trained on \"train_index\" and tested in \"test_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUC(X, y, train_index, test_index):\n",
    "    \"\"\"\n",
    "    feature/output: X, y\n",
    "    dataset split: train_index, test_index\n",
    "    \"\"\"\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "    clf = fit_logistic_regression(X_train, y_train)\n",
    "    default_proba_test = clf.predict_proba(X_test)[:,1]  \n",
    "    fpr, tpr, _ = roc_curve(y_test, default_proba_test)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return auc_score, fpr, tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: (5 min)\n",
    "\n",
    "With the help of the `compute_AUC` function defined above, write a function `cross_validation_AUC(X,y,nfold)` that carries out a 10-fold cross-validation and returns a list which contains the area under the curve for each fold of the cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_AUC(X,y, nfold=10):\n",
    "    \"\"\"\n",
    "    use a n-fold cross-validation for computing AUC estimates\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=nfold)  #create a cross-validation splitting\n",
    "    auc_list = [] #this list will contain the AUC estimates associated with each fold\n",
    "    for k, (train_index, test_index) in enumerate( skf.split(X, y) ):\n",
    "        auc_score, _, _ = compute_AUC(X, y, train_index, test_index)\n",
    "        auc_list.append(auc_score)\n",
    "    return auc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now estimate and compare, through cross-validation analysis, the performance of all the \"simple models\" that only use one numerical feature as input. As discussed in the EDA section, we will use the logarithmic transform for the `anual_income`, `loan_amount`, and `avg_cur_bal` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us extract only the numerical (i.e non-categorical) features\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "Data_numerics = Data.select_dtypes(include=numerics)\n",
    "Data_numerics = Data_numerics.drop([\"installment\", \"year\"], axis=1)\n",
    "\n",
    "# Using a log scale when appropriate\n",
    "Data_numerics[\"annual_inc\"] = np.log10(Data_numerics[\"annual_inc\"])\n",
    "Data_numerics[\"loan_amnt\"] = np.log10(Data_numerics[\"loan_amnt\"])\n",
    "Data_numerics[\"avg_cur_bal\"] = np.log10(1.+Data_numerics[\"avg_cur_bal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute cross-validation estimates of the AUC for each single-feature model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf = pd.DataFrame({}) #this data-frame will contain the AUC estimates\n",
    "for key in Data_numerics.keys():\n",
    "    if key == \"loan_default\": continue\n",
    "    X_full, y_full = Data_numerics[[key]], Data_numerics.loan_default\n",
    "    auc_list = cross_validation_AUC(X_full, y_full, nfold=10)\n",
    "    model_perf[\"SIMPLE:\" + key] = auc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: (5 min)\n",
    "\n",
    "Construct a [boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) which shows the distribution of cross-validation scores of each variable (remember, each variable has 10 total scores). Which feature has the highest/lowest predictive power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_ordered(df_model):\n",
    "    \"\"\"\n",
    "    display a list of boxplot, ordered by the media values\n",
    "    \"\"\"\n",
    "    df = df_model[df_model.median().sort_values().index]\n",
    "    sns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(df), showfliers=False)\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "plot_boxplot_ordered(model_perf)\n",
    "plt.xlabel(\"Predictive Model with a Single Predictive Feature\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: (5 min)\n",
    "\n",
    "Consider the model that consists of using *all* the numerical features (and none of the categorical features). Carry out a 10-fold cross-validation analysis to determine whether this model has better predictive performance than the best single-feature model. Use the boxplot method again as we did in Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, y_full = Data_numerics.drop([\"loan_default\"], axis=1), Data_numerics.loan_default\n",
    "auc_list = cross_validation_AUC(X_full, y_full)\n",
    "model_perf[\"ALL_NUMERICAL\"] = auc_list\n",
    "model_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "plot_boxplot_ordered(model_perf)\n",
    "plt.xlabel(\"Predictive Model with a Single Predictive Feature\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the combined model does perform better than the best single-feature model. Thus, we will move forward with it for the rest of this case. Note, however, that best practice would entail iteratively adding features to the best single-feature model until we reach a point where there is no significant improvement, as opposed to throwing all the features in at once. We advise you to take this more cautious approach when building your own models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating categorical variables (25 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grade of a loan (i.e. the LC-assigned loan grade feature) has not been used so far. The following is the distribution of the categorical grade feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.emp_length.value_counts()\n",
    "sns.countplot(x='grade', data = Data) \n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: (7 min)\n",
    "\n",
    "#### 8.1 (3 min)\n",
    "\n",
    "Use `pandas.get_dummies()` to transform this into its one-hot encoded version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a one-hot-encoding approach for incorporating the \"grade\" categorical variable\n",
    "grade_categ = pd.get_dummies(Data['grade'], prefix = \"grade\", drop_first=True)\n",
    "grade_categ.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 (4 min)\n",
    "\n",
    "Add this feature to the all-numerical model from earlier and investigate whether this leads to a significant increase in predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grade = pd.concat([X_full,grade_categ],axis=1) \n",
    "X_grade.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = cross_validation_AUC(X_grade, y_full)\n",
    "model_perf[\"ALL_NUMERICAL_WITH_GRADE\"] = auc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "plot_boxplot_ordered(model_perf)\n",
    "plt.xlabel(\"Predictive Model with a Single Predictive Feature\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference appears significant as the boxplot for the updated model is almost completely non-overlapping with that of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: (15 min)\n",
    "\n",
    "Investigate whether the categorical variable `job` brings any predictive value when added to the current best model. Again, you may want to use a one-hot encoding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "\n",
    "df_job_default = Data[[\"loan_default\", \"job\"]].groupby(\"job\").mean()\n",
    "df_job_default = df_job_default.sort_values(by=\"loan_default\",axis=0, ascending=False)\n",
    "sns.barplot(x=df_job_default.index[:50], \n",
    "            y=df_job_default[\"loan_default\"][:50].values,\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.ylabel(\"Defaut Probability\")\n",
    "plt.title(\"Default Rate per Employment Type\", fontsize=20, verticalalignment='bottom')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a one-hot-encoding approach for incorporating the \"purpose\" categorical variable\n",
    "job_categ = pd.get_dummies(Data['job'], prefix = \"job\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grade_job = pd.concat([X_grade,job_categ],axis=1) \n",
    "X_grade_job.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grade_job.keys()\n",
    "# --> 138 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_list = cross_validation_AUC(X_grade_job, y_full)\n",
    "model_perf[\"ALL_NUMERICAL_WITH_GRADE_JOB\"] = auc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "plot_boxplot_ordered(model_perf)\n",
    "plt.xlabel(\"Predictive Model with a Single Predictive Feature\")\n",
    "plt.ylabel(\"AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the boxplots overlap significantly, so there is no discernable benefit. We can repeat this process with other categorical variables to iteratively build the simplest possible model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions (5 min)\n",
    "\n",
    "In this case, we first explored the loan dataset and found the single-variable associations between the available features and the default rate. We also discovered which features required transformations (e.g. log transform).\n",
    "\n",
    "Once we started building models, we started with very simple logistic regressions approaches â€“ these baseline models were useful for quickly evaluating the predictive power of each individual variable. Next, we employed cross-validation approaches for building more complex models, often exploiting the interactions between the different features. Since the loan dataset contains a large number of covariates, using cross-validation was revealed to be crucial for avoiding overfitting, choosing the correct number of features and ultimately choosing an appropriate model that balanced complexity with accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways (3 min)\n",
    "\n",
    "Cross-validation is a robust and flexible technique for evaluating the predictive performance of statistical models. It is especially useful in big data settings where the number of features is large compared to the number of observations. When used appropriately, cross-validation is a powerful method for choosing a model with the correct complexity and best predictive performance. Remember that logistic regression is only one of many classification algorithms and the principles behind cross-validation are not limited to this case alone. In fact, we highly recommend utilizing cross-validation for your linear regression models as well to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
