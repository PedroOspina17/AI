{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can I compare different models that predict the probability of defaulting on a loan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "\n",
    "In this case, we will conduct a comprehensive walkthrough of a binary classification data science problem. We aim to combine the various pre-modeling techniques we hae learned with **cross-validation** to tune a logistic regression model.\n",
    "\n",
    "Additionally, we introduce various metrics (ROC/AUC) to evaluate the performance of our classification model which is inherently different from the previous linear regression models we have encountered. This goal may seem a bit daunting or technical but by the end of the case, all these terms should be clear to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load relevant packages\n",
    "import pandas                  as pd\n",
    "import numpy                   as np\n",
    "import matplotlib.pyplot       as plt\n",
    "import seaborn                 as sns\n",
    "import os\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Business Context.** Traditional commercial banks typically did not rely on statistical modeling to decide whether personal loans should be issued, although this is changing rapidly nowadays. You are a data scientist working in a modern commercial bank. Your data science team has already built simple regression models for predicting the probability that those loans would be defaulted on. However, you have noticed that many of these models perform much worse in production than they do in testing.\n",
    "\n",
    "**Business Problem.** Your task is to **build a default probability model that you feel comfortable putting into production**.\n",
    "\n",
    "**Analytical Context.** The dataset contains the details of 5000 loan requests that have been previously issued by your bank. For each loan, the final status of the loan (i.e. whether the loan defaulted) is also available:\n",
    "\n",
    "1. The file **\"loan_light.csv\"** contains the details of 5000 loans\n",
    "2. The file **\"loan_param.xlsx\"** contains the description of each covariate\n",
    "\n",
    "The case will proceed as follows: you will 1) perform some data exploration to determine the appropriate variable transformations to make; 2) fit some simple models; 3) learn about **cross-validation** and use this to select the best simple model; and finally 4) responsibly construct more complex models using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n",
    "Let's start by taking a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"loan_light.csv\")\n",
    "Data = Data.sample(frac=1)  #shuffle the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>application_type</th>\n",
       "      <th>avg_cur_bal</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>dti</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>grade</th>\n",
       "      <th>inq_last_12m</th>\n",
       "      <th>installment</th>\n",
       "      <th>...</th>\n",
       "      <th>num_actv_bc_tl</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>term</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>num_tl_90g_dpd_24m</th>\n",
       "      <th>purpose</th>\n",
       "      <th>year</th>\n",
       "      <th>loan_default</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3316</th>\n",
       "      <td>51500.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>13864.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "      <td>0.0</td>\n",
       "      <td>202.26</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>teacher</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>48000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>10267.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.15</td>\n",
       "      <td>7</td>\n",
       "      <td>D</td>\n",
       "      <td>4.0</td>\n",
       "      <td>722.95</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>36</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>63600.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>28891.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.11</td>\n",
       "      <td>5</td>\n",
       "      <td>B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255.04</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>36</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4182</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>Individual</td>\n",
       "      <td>6882.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.98</td>\n",
       "      <td>9</td>\n",
       "      <td>C</td>\n",
       "      <td>2.0</td>\n",
       "      <td>236.30</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>60</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>85000.0</td>\n",
       "      <td>Joint App</td>\n",
       "      <td>21065.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.42</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1210.72</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>36</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>debt_consolidation</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>examiner</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      annual_inc application_type  avg_cur_bal  chargeoff_within_12_mths  \\\n",
       "3316     51500.0       Individual      13864.0                       0.0   \n",
       "779      48000.0       Individual      10267.0                       0.0   \n",
       "916      63600.0       Individual      28891.0                       0.0   \n",
       "4182     60000.0       Individual       6882.0                       0.0   \n",
       "1613     85000.0        Joint App      21065.0                       0.0   \n",
       "\n",
       "      delinq_2yrs    dti  emp_length grade  inq_last_12m  installment  ...  \\\n",
       "3316          0.0  20.60           2     C           0.0       202.26  ...   \n",
       "779           0.0  34.15           7     D           4.0       722.95  ...   \n",
       "916           0.0  21.11           5     B           1.0       255.04  ...   \n",
       "4182          1.0  21.98           9     C           2.0       236.30  ...   \n",
       "1613          0.0  12.42           4     C           1.0      1210.72  ...   \n",
       "\n",
       "      num_actv_bc_tl  pub_rec_bankruptcies  home_ownership term  mort_acc  \\\n",
       "3316             1.0                   0.0            RENT   36       0.0   \n",
       "779              6.0                   0.0        MORTGAGE   36       2.0   \n",
       "916              3.0                   0.0        MORTGAGE   36       6.0   \n",
       "4182             4.0                   1.0        MORTGAGE   60       4.0   \n",
       "1613             4.0                   0.0        MORTGAGE   36       7.0   \n",
       "\n",
       "      num_tl_90g_dpd_24m             purpose  year  loan_default       job  \n",
       "3316                 0.0  debt_consolidation  2017             1   teacher  \n",
       "779                  0.0  debt_consolidation  2016             0   manager  \n",
       "916                  0.0  debt_consolidation  2016             0   manager  \n",
       "4182                 0.0  debt_consolidation  2016             1    worker  \n",
       "1613                 0.0  debt_consolidation  2016             0  examiner  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['annual_inc', 'application_type', 'avg_cur_bal',\n",
       "       'chargeoff_within_12_mths', 'delinq_2yrs', 'dti', 'emp_length', 'grade',\n",
       "       'inq_last_12m', 'installment', 'loan_amnt', 'num_actv_bc_tl',\n",
       "       'pub_rec_bankruptcies', 'home_ownership', 'term', 'mort_acc',\n",
       "       'num_tl_90g_dpd_24m', 'purpose', 'year', 'loan_default', 'job'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-45ba5d518a87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_description\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loan_param.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf_description\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'width'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'1000px'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DS4A\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, **kwds)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DS4A\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, io, engine)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DS4A\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \"\"\"\n\u001b[0;32m     19\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\DS4A\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, raise_on_missing, on_version)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraise_on_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "df_description = pd.read_excel('loan_param.xlsx').dropna()\n",
    "df_description.style.set_properties(subset=['Description'], **{'width': '1000px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "\n",
    "For each of the following, perform the directed visualization and discuss your conclusions from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1\n",
    "\n",
    "Create a [bar chart](https://www.mathsisfun.com/data/bar-graphs.html) showing the number of loans that did and did not default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2\n",
    "\n",
    "Plot a [histogram](https://www.mathsisfun.com/data/histograms.html) of the annual incomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is quite skewed. Let's try a [logarithmic transformation](https://dev.to/rokaandy/logarithmic-transformation-in-linear-regression-models-why-when-3a7c) to get the data to be more normally distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(Data.annual_inc).hist(bins=100, density=True)\n",
    "plt.title(\"Annual Income Distribution\")\n",
    "plt.xlabel(\"Annual Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3\n",
    "\n",
    "Is the distribution of annual incomes different between applicants who defaulted vs. applicants who did not default on their loans?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4\n",
    "\n",
    "Explore the association between annual income and the monthly installment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few more figures which look at the relationship between other numerical covariates and the probability of default, as well as annual income:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`emp_length`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize = (10,5), ncols=2, sharey= False)\n",
    "sns.boxplot(x='emp_length', y = 'annual_inc', data = Data, showfliers=False, ax = ax1) #showfliers=False for nice display\n",
    "ax1.set_title(\"Employment length vs annual income\")\n",
    "Data[[\"emp_length\",'loan_default']].groupby(\"emp_length\").mean().plot.bar(rot=90,ax = ax2)\n",
    "plt.title(\"Default probability vs. home ownership\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`homeOwnership`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize = (10,5), ncols=2, sharey= False)\n",
    "sns.boxplot(x=\"home_ownership\",y=\"annual_inc\", data = Data, showfliers=False, ax = ax1) #showfliers=False for nice display\n",
    "ax1.set_title(\"Home ownership vs. annual income\")\n",
    "Data[[\"home_ownership\",'loan_default']].groupby(\"home_ownership\").mean().plot.bar(rot=90,ax = ax2)\n",
    "plt.title(\"Default probability vs. home ownership\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some figures that show the relationship between various categorical variables and the probability of default:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`purpose`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "Data.emp_length.value_counts()\n",
    "sns.countplot(x='purpose', order=Data['purpose'].value_counts().index, data = Data) \n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribution of Loan Purposes\", fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10,5))\n",
    "purpose_default = Data[[\"loan_default\", \"purpose\"]].groupby(\"purpose\").mean()\n",
    "purpose_default = purpose_default.sort_values(by=\"loan_default\",axis=0, ascending=False)\n",
    "sns.barplot(x=purpose_default.index[:30], \n",
    "            y=purpose_default[\"loan_default\"][:30].values,\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Default Probability\");\n",
    "plt.title(\"Default Probability by Loan Purpose\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`job`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15,5))\n",
    "sns.barplot(x=Data[\"job\"].value_counts()[:30].index.values , \n",
    "            y=100 * Data.job.value_counts()[:30].values / len(Data),\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Percentage of Population\")\n",
    "plt.title(\"Distribution of Jobs\", fontsize=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (20,5))\n",
    "\n",
    "df_job_default = Data[[\"loan_default\", \"job\"]].groupby(\"job\").mean()\n",
    "df_job_default = df_job_default.sort_values(by=\"loan_default\",axis=0, ascending=False)\n",
    "sns.barplot(x=df_job_default.index[:50], \n",
    "            y=df_job_default[\"loan_default\"][:50].values,\n",
    "            orient=\"v\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.ylabel(\"Defaut Probability\")\n",
    "plt.title(\"Default Probability by Job Type\", fontsize=20, verticalalignment='bottom');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new variable\n",
    "\n",
    "The yearly payment owed by the borrower, as a fraction of their annual income, is a standard metric used in evaluating whether a loan should be issued. Let's define a new variable **\"install_income\"** which codes the installment as a fraction of the annual income and study its association with the other features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data['install_income'] = 12 * Data.installment / Data.annual_inc\n",
    "H = plt.hist(Data['install_income'], bins=100, density=True)\n",
    "plt.xlabel(r\"Installment / Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to easily investigate this variable's association with the probability of default, define a new covariate named `install_income_disc` that is a discretized version of `install_income`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us discretize the \"install_income\" variable to study the probability of default \n",
    "# as a function of \"install_income\"\n",
    "Data[\"install_income_disc\"] = (Data.install_income*50).astype(int)/50.  #discretization\n",
    "Data[[\"loan_default\", \"install_income_disc\"]].groupby(\"install_income_disc\").mean().plot.bar(rot=90)\n",
    "Data = Data.drop([\"install_income_disc\"], axis=1)\n",
    "\n",
    "# --> there is a clear positive association: as the fraction of the annual income devoted to the re-imbursement of \n",
    "# the loan increases, the probability of default sharply increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "\n",
    "Visualize the [correlation matrix](https://www.statisticshowto.com/correlation-matrix/) across all numerical features by using the `sns.heatmap()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute correlation matrix\n",
    "df_correlations = Data.corr()\n",
    "\n",
    "#mask the upper half for visualization purposes\n",
    "mask = np.zeros_like(df_correlations, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "plt.figure(figsize= (10,10))\n",
    "\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(df_correlations,mask=mask,  vmax=1, vmin=-1, cmap=cmap, \n",
    "            center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that `installment` and `loan_amnt` are highly correlated. In general, we can use a correlation matrix as an early feature selection technique to remove features that are extremely correlated to account for multicollinearity. There is no scientific cutoff for when a correlation warrants removing a feature so always investigate further. Think of it as a red flag moment when you see highly correlated parameters and then investigate the reason behind such a correlation to see if one of the features can be removed. A classic example of correlated features that require removal would be two features displaying distance in miles and kilometers, respectively. There is clearly redundant information there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a predictive model\n",
    "\n",
    "Let's first start by building a standard [logistic regression model](https://www.youtube.com/watch?v=yIYKR4sgzI8). In general, it is important and extremely useful to first create baseline/simple models which can be compared to more complex models later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "#### 3.1\n",
    "\n",
    "Using the `LogisticRegression()` function from `scikit-learn`, write a function named `fit_logistic_regression(X,y)` that fits a logistic regression on the array of covariates `X` and associated response variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def fit_logistic_regression(X,y):\n",
    "    \"\"\"\n",
    "    fit a logistic regression with feature matrix X and binary output y\n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(solver='lbfgs', tol=10**-4,  \n",
    "                             fit_intercept=True, \n",
    "                             multi_class='multinomial').fit(X,y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2\n",
    "\n",
    "Create a basic [logistic regression model](https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) for predicting the loan default with only one feature: `install_income`.  Call this model `model1`. Use a 70/30 train-test split of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "X_ind = np.choose.random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fitted a model, we will proceed to evaluate how \"good\" it is. Classification models are judged differently than linear regression models. A common tool you will have to get used to is the [confusion matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62). Additionally, we will introduce some other metrics related to classification algorithms below. It is important to note these metrics apply to other classification models, not just logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3\n",
    "\n",
    "Plot the [ROC curve](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) of `model1` and find the area under the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "#### 4.1\n",
    "\n",
    "Consider `model1` from above. Would you want this to be your final model? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2\n",
    "\n",
    "Let's instead put all the variables available in the model, so that we are maximally leveraging our available info. Would you be in favor of this or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, you will be working with datasets with many features that each have their own distribution. Generally, a large amount of time is spent on feature selection with many models being trained during this time. It is extremely rare that you simply plug all the features in and tune it once to get the optimal model. \n",
    "    \n",
    "There are many different techniques associated with feature selection and a comprehensive look into all of them is outside the scope of this case. For simplicity, we will demonstrate model training and testing on single-feature models and then directly move into multi-feature models to show the numerous possible cases you may encounter. In reality, we would apply cross-validation on numerous subsets of features based on domain knowledge of the dataset to see which set of features truly optimizes the model we are trying to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Cross-validation**](https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f) is a set of techniques for assessing how well the results of a model will generalize to an out-of-sample dataset; i.e. in practice or production. It is chiefly used to flag overfitting.\n",
    "\n",
    "Cross-validation works as follows: one splits the available data into $k$ sets, or **folds**. $k - 1$ of these folds will be used to train the model, while the held-out fold will be used as the test set on which the model is evaluated. For computational stability, this procedure is generally split many times, such that each fold has an opportunity to serve as the test set. For each repetition, a metric of prediction performance (e.g. AUC) is calculated on the test set. The average of these metrics, as well as their standard deviation, is then reported. \n",
    "\n",
    "There is no exact science for choosing a proper $k$ for your dataset. It depends on what type of data you are using and how large it is. The larger $k$ is, the most iterations you run which lowers the chance you get \"unlucky\"; however, running more iterations can be costly in some cases. Additionally, it is important to make sure no matter what $k$ you choose that the validation set is still large enough to be meaningful. If you have a data set of 100 points and choose a $k$ of 20, then the validation set will only be 5 points. It is likely some iterations will perform poorly simply because the validation set is too small and potentially too different from the training set.\n",
    "\n",
    "An example is shown here for 5-fold cross-validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](cv_fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this with code. The following code displays the 5 different folds used in a standard 5-fold cross-validation approach. To do so, use the `StratifiedKFold()` function from `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "for k, (train_index, test_index) in enumerate( skf.split(X, y) ):\n",
    "    plt.plot(train_index, [k+1 for _ in train_index], \".\")\n",
    "plt.ylim(0,6)\n",
    "plt.ylabel(\"FOLD\")\n",
    "plt.title(\"CROSS VALIDATION FOLDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code defines a function `compute_AUC(X, y, train_index, test_index)` that computes the AUC of a model trained on \"train_index\" and tested in \"test_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUC(X, y, train_index, test_index):\n",
    "    \"\"\"\n",
    "    feature/output: X, y\n",
    "    dataset split: train_index, test_index\n",
    "    \"\"\"\n",
    "    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n",
    "    X_test, y_test = X.iloc[test_index], y.iloc[test_index]\n",
    "\n",
    "    clf = fit_logistic_regression(X_train, y_train)\n",
    "    default_proba_test = clf.predict_proba(X_test)[:,1]  \n",
    "    fpr, tpr, _ = roc_curve(y_test, default_proba_test)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    return auc_score, fpr, tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "With the help of the `compute_AUC` function defined above, write a function `cross_validation_AUC(X,y,nfold)` that carries out a 10-fold cross-validation and returns a list which contains the area under the curve for each fold of the cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now estimate and compare, through cross-validation analysis, the performance of all the \"simple models\" that only use one numerical feature as input. As discussed in the EDA section, we will use the logarithmic transform for the `anual_income`, `loan_amount`, and `avg_cur_bal` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us extract only the numerical (i.e non-categorical) features\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "Data_numerics = Data.select_dtypes(include=numerics)\n",
    "Data_numerics = Data_numerics.drop([\"installment\", \"year\"], axis=1)\n",
    "\n",
    "# Using a log scale when appropriate\n",
    "Data_numerics[\"annual_inc\"] = np.log10(Data_numerics[\"annual_inc\"])\n",
    "Data_numerics[\"loan_amnt\"] = np.log10(Data_numerics[\"loan_amnt\"])\n",
    "Data_numerics[\"avg_cur_bal\"] = np.log10(1.+Data_numerics[\"avg_cur_bal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute cross-validation estimates of the AUC for each single-feature model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_perf = pd.DataFrame({}) #this data-frame will contain the AUC estimates\n",
    "for key in Data_numerics.keys():\n",
    "    if key == \"loan_default\": continue\n",
    "    X_full, y_full = Data_numerics[[key]], Data_numerics.loan_default\n",
    "    auc_list = cross_validation_AUC(X_full, y_full, nfold=10)\n",
    "    model_perf[\"SIMPLE:\" + key] = auc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "Construct a [boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) which shows the distribution of cross-validation scores of each variable (remember, each variable has 10 total scores). Which feature has the highest/lowest predictive power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7:\n",
    "\n",
    "Consider the model that consists of using *all* the numerical features (and none of the categorical features). Carry out a 10-fold cross-validation analysis to determine whether this model has better predictive performance than the best single-feature model. Use the boxplot method again as we did in Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grade of a loan (i.e. the LC-assigned loan grade feature) has not been used so far. The following is the distribution of the categorical grade feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.emp_length.value_counts()\n",
    "sns.countplot(x='grade', data = Data) \n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8:\n",
    "\n",
    "#### 8.1\n",
    "\n",
    "Use `pandas.get_dummies()` to transform this into its one-hot encoded version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2\n",
    "\n",
    "Add this feature to the all-numerical model from earlier and investigate whether this leads to a significant increase in predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9:\n",
    "\n",
    "Investigate whether the categorical variable `job` brings any predictive value when added to the current best model. Again, you may want to use a one-hot encoding scheme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this case, we first explored the loan dataset and found the single-variable associations between the available features and the default rate. We also discovered which features required transformations (e.g. log transform).\n",
    "\n",
    "Once we started building models, we started with very simple logistic regressions approaches – these baseline models were useful for quickly evaluating the predictive power of each individual variable. Next, we employed cross-validation approaches for building more complex models, often exploiting the interactions between the different features. Since the loan dataset contains a large number of covariates, using cross-validation was revealed to be crucial for avoiding overfitting, choosing the correct number of features and ultimately choosing an appropriate model that balanced complexity with accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Cross-validation is a robust and flexible technique for evaluating the predictive performance of statistical models. It is especially useful in big data settings where the number of features is large compared to the number of observations. When used appropriately, cross-validation is a powerful method for choosing a model with the correct complexity and best predictive performance. Remember that logistic regression is only one of many classification algorithms and the principles behind cross-validation are not limited to this case alone. In fact, we highly recommend utilizing cross-validation for your linear regression models as well to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
