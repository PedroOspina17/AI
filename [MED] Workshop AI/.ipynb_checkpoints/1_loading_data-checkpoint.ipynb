{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the Natural Language Modelling\n",
    "\n",
    "NLP, a.k.a. Natural Language Programming or Processing or Modelling..., is defined as the systematic way of to make any text understandable for machines. Moreover, NLP deals with the enriched world of languages in order to allow machines comprehend and communicate like any other human.\n",
    "\n",
    "Asides from rhetoric, NLP aims at doing this:\n",
    "\n",
    "![corpus](images/1_purpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "We are going to work with [wikipedia dumps files](https://dumps.wikimedia.org/), which are a collection of articles dumped every day and in different languages. To be practical, we're going to use English articles. To save you time, effort and headaches, a bunch of dumped files have been downloaded for you. The [wikiextractor tool](http://medialab.di.unipi.it/wiki/Wikipedia_Extractor) was used to extract dumps packages into a set of flat text files.\n",
    "\n",
    "### Basic imports for this module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import operator\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = text.lower()  # downcase\n",
    "    return text.split()\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # Replace punctuation with tokens so we can use them in our model\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace(',', '')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(';', '')\n",
    "    text = text.replace('!', '')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('-', ' ')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    \n",
    "    return text.translate(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for getting wikipedia data into the workspace memory, not yours but your computer's one\n",
    "\n",
    "Do you happen to know what really reading text files in NLP means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_data(n_files):\n",
    "    prefix = './wikifiles/'\n",
    "\n",
    "    # checking existance of folder with text files downloaded from wikipedia\n",
    "    if not os.path.exists(prefix):\n",
    "        print(\"Are you sure you've downloaded, converted, and placed the Wikipedia data into the proper folder (wikifiles)?\")\n",
    "        print(\"Quitting...\")\n",
    "        exit()\n",
    "\n",
    "    # getting list of files from folder and subfolders\n",
    "    input_files = []\n",
    "    for folder in os.listdir(prefix):\n",
    "        for f in os.listdir(prefix + \"/\" + folder):\n",
    "            if f.startswith('wiki'):\n",
    "                input_files.append(\"/\" + folder + \"/\" + f)\n",
    "\n",
    "    if len(input_files) == 0:\n",
    "        print(\"Looks like you don't have any data files, or they're in the wrong location.\")\n",
    "        print(\"Please download the data from https://dumps.wikimedia.org/\")\n",
    "        print(\"Quitting...\")\n",
    "        exit()\n",
    "\n",
    "    # list of sentences\n",
    "    sentences = []\n",
    "    \n",
    "    # initializing dictionaries of words\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "\n",
    "    # shuflling files\n",
    "    random.shuffle(input_files)\n",
    "    \n",
    "    if n_files is not None:\n",
    "        input_files = input_files[:n_files]\n",
    "        \n",
    "    # for each file, reads its sentences and cleanses them\n",
    "    for f in input_files:\n",
    "        #print(\"reading:\", f)\n",
    "        for line in open(prefix + \"/\" + f):\n",
    "            line = line.strip()\n",
    "            # don't count headers, structured data, lists, etc...\n",
    "            if line and line[0] not in ('[', '*', '-', '|', '=', '{', '}', '<', '/', '\\\\'):\n",
    "                # sentence into a list of words\n",
    "                sentence_lines = line.split('. ')\n",
    "                \n",
    "                for sentence in sentence_lines:\n",
    "                    # cleasing sentence to get a list of words\n",
    "                    tokens = my_tokenizer(sentence)\n",
    "                    # each word is indexed and counted\n",
    "                    for t in tokens:\n",
    "                        if t not in word2idx:\n",
    "                            word2idx[t] = current_idx\n",
    "                            idx2word.append(t)\n",
    "                            current_idx += 1    \n",
    "                        idx = word2idx[t]\n",
    "                        # counting\n",
    "                        word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "                    # replacing sentence with indexes of words\n",
    "                    sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "                    sentences.append(sentence_by_idx)\n",
    "    \n",
    "    with open('1.1_read_wikis.pickle', 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump((word_idx_count, idx2word, sentences), f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return word_idx_count, idx2word, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><img src=\"images/task.png\" width=\"5%\" height=\"5%\">\n",
    "\n",
    ">**Exercise:** Below, execute get_wikipedia_data specifying a number of files up to 200 (otherwise we wouln't go home today). Notice that the function returns three elements, check them all a little bit and see in what all of those wiki files have been turned into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx, idx2word, sentences = get_wikipedia_data(n_files=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3106493"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'constitution',\n",
       " 'of',\n",
       " 'mongolia',\n",
       " 'provides',\n",
       " 'for',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'religion',\n",
       " 'and',\n",
       " 'the',\n",
       " 'mongolian',\n",
       " 'government',\n",
       " 'generally',\n",
       " 'respects',\n",
       " 'this',\n",
       " 'right',\n",
       " 'in',\n",
       " 'practice',\n",
       " 'however',\n",
       " 'the',\n",
       " 'law',\n",
       " 'somewhat',\n",
       " 'limits',\n",
       " 'proselytism',\n",
       " 'and',\n",
       " 'some',\n",
       " 'religious',\n",
       " 'groups',\n",
       " 'have',\n",
       " 'faced',\n",
       " 'bureaucratic',\n",
       " 'harassment',\n",
       " 'or',\n",
       " 'been',\n",
       " 'denied',\n",
       " 'registration']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2word[word] for word in sentences[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling... as always\n",
    "\n",
    "Well, if you guessed it, we've juts created a vocabulary! ...and by vocabulary I don't mean just numbers, vowels and consonants.\n",
    "\n",
    "In some cases, corpora comprise vast amount of information. Imposible to be processed in just one go. Particulary, sampling is the best way to extract a piece of information that should represent quite well the whole dataset. It means that in somehow a text representation technique is needed in order to make a selection and then an extraction of a representative small dataset to deal with.\n",
    "\n",
    "To do so, the simplest and easiest technique of human kind is applied: counting!\n",
    "\n",
    "...but to be fancier lets just called as bag of words, then n-grams.\n",
    "\n",
    "Vocabulary turns into features for a Machine Learning model. Let's say that here we have up 2000 terms in 1000 files (documents), which makes it a matrix of 2000x1000 (tiny big).\n",
    "\n",
    "But... lets imagine what if a machine tries to learn about the whole set of words in a language...\n",
    "\n",
    "__The Second Edition of the 20-volume Oxford English Dictionary contains full entries for 171,476 words in current use, and 47,156 obsolete words. To this may be added around 9,500 derivative words included as subentries.__\n",
    "\n",
    "In a bigram model that means a matrix of at least 171476x171476, and that's a lot. An average human cannot learn so much words neither."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampling(word2idx, idx2word, sentences, n_vocab):\n",
    "    # restrict vocab size\n",
    "    sorted_word_idx_count = sorted(word2idx.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    \n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "        \n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx\n",
    "    idx2word_small = {v:k for k,v in word2idx_small.items()}\n",
    "    unknown = new_idx\n",
    "    \n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 0:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "              \n",
    "    with open('1.2_subsampled_wikis.pickle', 'wb') as f:\n",
    "        # Pickle the 'data' dictionary using the highest protocol available.\n",
    "        pickle.dump((word2idx_small, idx2word_small, sentences_small), f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return word2idx_small, idx2word_small, sentences_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx_sampled, idx2word_sampled, sentences_sampled  = subsampling(word2idx, idx2word, sentences, n_vocab=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'constitution',\n",
       " 'of',\n",
       " 'mongolia',\n",
       " 'provides',\n",
       " 'for',\n",
       " 'freedom',\n",
       " 'of',\n",
       " 'religion',\n",
       " 'and',\n",
       " 'the',\n",
       " 'UNKNOWN',\n",
       " 'government',\n",
       " 'generally',\n",
       " 'UNKNOWN',\n",
       " 'this',\n",
       " 'right',\n",
       " 'in',\n",
       " 'practice',\n",
       " 'however',\n",
       " 'the',\n",
       " 'law',\n",
       " 'somewhat',\n",
       " 'limits',\n",
       " 'UNKNOWN',\n",
       " 'and',\n",
       " 'some',\n",
       " 'religious',\n",
       " 'groups',\n",
       " 'have',\n",
       " 'faced',\n",
       " 'UNKNOWN',\n",
       " 'UNKNOWN',\n",
       " 'or',\n",
       " 'been',\n",
       " 'denied',\n",
       " 'registration']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[idx2word_sampled[word] for word in sentences_sampled[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><img src=\"images/task.png\" width=\"5%\" height=\"5%\">\n",
    "\n",
    "> **Exercise (OPTIONAL):** Create another sampling method. Instead of receiving a parameter for a maximum number of vocabulary, it should receive a minimum percentage of words out of the complete vocabulary to be contained in the sample."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
